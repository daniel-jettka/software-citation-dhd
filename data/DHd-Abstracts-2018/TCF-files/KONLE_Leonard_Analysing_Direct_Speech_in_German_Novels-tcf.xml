<?xml version='1.0' encoding='UTF-8'?><D-Spin xmlns="http://www.dspin.de/data" version="5">
  <MetaData xmlns="http://www.dspin.de/data/metadata"><Services><cmd:CMD xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:cmd="http://www.clarin.eu/cmd/1" CMDVersion="1.2" xsi:schemaLocation="http://www.clarin.eu/cmd/1 http://catalog.clarin.eu/ds/ComponentRegistry/rest/registry/profiles/clarin.eu:cr1:p_1320657629623/xsd"><cmd:Resources><cmd:ResourceProxyList/><cmd:JournalFileProxyList/><cmd:ResourceRelationList/></cmd:Resources><cmd:Components><cmd:WebServiceToolChain><cmd:GeneralInfo><cmd:Descriptions><cmd:Description/></cmd:Descriptions><cmd:ResourceName>Custom chain</cmd:ResourceName><cmd:ResourceClass>Toolchain</cmd:ResourceClass></cmd:GeneralInfo><cmd:Toolchain><cmd:ToolInChain><cmd:PID>https://hdl.handle.net/21.11120/0000-0008-319A-3</cmd:PID><cmd:Parameter value="de" name="lang"/></cmd:ToolInChain><cmd:ToolInChain><cmd:PID>https://hdl.handle.net/21.11120/0000-0008-3183-C</cmd:PID><cmd:Parameter value="5" name="version"/></cmd:ToolInChain><cmd:ToolInChain><cmd:PID>http://hdl.handle.net/11022/0000-0007-DA29-6</cmd:PID><cmd:Parameter value="de" name="lang"/><cmd:Parameter value="5" name="version"/></cmd:ToolInChain></cmd:Toolchain></cmd:WebServiceToolChain></cmd:Components></cmd:CMD></Services></MetaData>
  <TextCorpus xmlns="http://www.dspin.de/data/textcorpus" lang="de">
    <textSource type="application/tei+xml;format-variant=tei-dta;tokenized=0">&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;TEI xmlns="http://www.tei-c.org/ns/1.0"
     xml:id="KONLE_Leonard_Analysing_Direct_Speech_in_German_Novels">
&lt;teiHeader>
&lt;fileDesc>
&lt;titleStmt>
&lt;title type="full">
&lt;title type="main">Analysing Direct Speech in German Novels&lt;/title>
&lt;title type="sub"/>
&lt;/title>
&lt;author>
&lt;persName>
&lt;surname>Jannidis&lt;/surname>
&lt;forename>Fotis&lt;/forename>
&lt;/persName>
&lt;affiliation>Universität Würzburg, Deutschland&lt;/affiliation>
&lt;email>fotis.jannidis@uni-wuerzburg.de&lt;/email>
&lt;/author>
&lt;author>
&lt;persName>
&lt;surname>Konle&lt;/surname>
&lt;forename>Leonard&lt;/forename>
&lt;/persName>
&lt;affiliation>Universität Würzburg, Deutschland&lt;/affiliation>
&lt;email>leonard.konle@uni-wuerzburg.de&lt;/email>
&lt;/author>
&lt;author>
&lt;persName>
&lt;surname>Zehe&lt;/surname>
&lt;forename>Albin&lt;/forename>
&lt;/persName>
&lt;affiliation>Universität Würzburg, Deutschland&lt;/affiliation>
&lt;email>zehe@informatik.uni-wuerzburg.de&lt;/email>
&lt;/author>
&lt;author>
&lt;persName>
&lt;surname>Hotho&lt;/surname>
&lt;forename>Andreas&lt;/forename>
&lt;/persName>
&lt;affiliation>Universität Würzburg, Deutschland&lt;/affiliation>
&lt;email>hotho@informatik.uni-wuerzburg.de&lt;/email>
&lt;/author>
&lt;author>
&lt;persName>
&lt;surname>Krug&lt;/surname>
&lt;forename>Markus&lt;/forename>
&lt;/persName>
&lt;affiliation>Universität Würzburg, Deutschland&lt;/affiliation>
&lt;email>markus.krug@uni-wuerzburg.de&lt;/email>
&lt;/author>
&lt;/titleStmt>
&lt;editionStmt>
&lt;edition>
&lt;date>2018-01-14T14:08:20.834438720&lt;/date>
&lt;/edition>
&lt;/editionStmt>
&lt;publicationStmt>
&lt;t:publisher xmlns:t="http://www.tei-c.org/ns/1.0">Georg Vogeler, im Auftrag des Verbands Digital Humanities im deutschaprachigen Raum e.V.&lt;/t:publisher>
&lt;t:address xmlns:t="http://www.tei-c.org/ns/1.0">
&lt;t:addrLine>Universität Graz&lt;/t:addrLine>
&lt;t:addrLine>Zentrum für Informationsmodellierung - Austrian Centre for Digital Humanities&lt;/t:addrLine>
&lt;t:addrLine>Elisabethstraße 59/III&lt;/t:addrLine>
&lt;t:addrLine>8010 Graz&lt;/t:addrLine>
&lt;/t:address>
&lt;/publicationStmt>
&lt;sourceDesc>
&lt;p>Converted from an OASIS Open Document&lt;/p>
&lt;/sourceDesc>
&lt;/fileDesc>
&lt;encodingDesc>
&lt;appInfo>
&lt;application ident="DHCONVALIDATOR" version="1.17">
&lt;label>DHConvalidator&lt;/label>
&lt;/application>
&lt;/appInfo>
&lt;/encodingDesc>
&lt;profileDesc>
&lt;textClass>
&lt;keywords scheme="ConfTool" n="category">
&lt;term>Paper&lt;/term>
&lt;/keywords>
&lt;keywords scheme="ConfTool" n="subcategory">
&lt;term>Vortrag&lt;/term>
&lt;/keywords>
&lt;keywords scheme="ConfTool" n="keywords">
&lt;term>Direct Speech&lt;/term>
&lt;term>Deep Learning&lt;/term>
&lt;term>Machine Learning&lt;/term>
&lt;/keywords>
&lt;keywords scheme="ConfTool" n="topics">
&lt;term>Datenerkennung&lt;/term>
&lt;term>Programmierung&lt;/term>
&lt;term>Stilistische Analyse&lt;/term>
&lt;term>Text&lt;/term>
&lt;/keywords>
&lt;/textClass>
&lt;settingDesc>
&lt;ab n="conference">DHd2018 - "Kritik der Digitalen Vernunft", Köln&lt;/ab>
&lt;ab n="paperID">227&lt;/ab>
&lt;ab n="session_ID">115&lt;/ab>
&lt;ab n="session_numberInSession">2&lt;/ab>
&lt;ab n="session_short">VP_4a&lt;/ab>
&lt;ab n="session_title">Digitale Literaturwissenschaft&lt;/ab>
&lt;ab n="session_start">2018-03-01 09:00&lt;/ab>
&lt;ab n="session_end">2018-03-01 10:30&lt;/ab>
&lt;/settingDesc>
&lt;/profileDesc>
&lt;/teiHeader>
&lt;text>
&lt;body>
&lt;div type="div1" rend="DH-Heading1">
&lt;head>
&lt;anchor xml:id="id_docs-internal-guid-7388127a-f4c9-8ec4-0920-166c6bf72522"/>Introduction
                &lt;/head>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-5e3754e0-f5cb-94e5-6b7e-e4f84c333592"/>Detecting direct speech in fiction allows gaining insight into an important element of its narrative structure. In literary studies, there are assumptions on the factors influencing the distribution of direct speech, like genre, period and aesthetic complexity.
                &lt;/p>
&lt;p>This paper aims to provide a detailed analysis of the use of direct speech across different time periods and domains. To create a reliable database for these analyses, we need to measure the usage of direct speech in a large and representative corpus. This task is more challenging than it may sound: While, nowadays, direct speech is often marked very explicitly by the use of quotes, this has not always been consistently the case. Many historical novels are not available in a well-edited form, meaning that there may be inconsistent use of quotation, or no quotation at all (Brunner, 2013). In this case, a more robust method for detecting direct speech is necessary.&lt;/p>
&lt;p>Our first contribution is therefore a deep learning-based method to detect direct speech using large amounts of rule-based, but slightly flawed, labelled data extracted from raw text.  This has multiple advantages over the use of manually annotated training data: First, manually annotating large amounts of text is very time-intensive and therefore costly. Furthermore, annotations for one type of texts may not be transferable to other types, leading to the necessity of new annotated data for new corpora. Being able to learn from the already existing weakly labelled data is therefore desirable, as this data can automatically be extracted for a new corpus.&lt;/p>
&lt;p>Our second contribution is the application of this approach on curated texts to gain insight in trends of direct speech distribution. On one hand we try to look for development of direct speech over time, analysing a large dataset of novels from the nineteenth century, on the other hand we focus on differences in genre comparing contemporary high and low brow literature.&lt;/p>
&lt;/div>
&lt;div type="div1" rend="DH-Heading1">
&lt;head>Related Work and Task Description&lt;/head>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-d44f6f66-f5cc-1464-f0a8-9c54aaf8310d"/>There have been several previous approaches to direct speech detection applying machine learning methods.
                &lt;/p>
&lt;p>For example, Brunner (2013) tests rule-based and machine learning driven classification, as well as combinations of both, on German novels. She recommends using a pure machine learning approach (Random Forest), reaching an F1 score of 0.87.&lt;/p>
&lt;p>Scheible et al. (2016) employ a simple greedy algorithm and a semi-Markov model, showing that the latter outperforms the previous state-of-the-art by achieving a precision of 0.88.&lt;/p>
&lt;p>Although the results seem quite satisfying, these systems require a relatively large amount of labelled data for training. As stated above, this is problematic because of the need for expensive annotation and lack of transferability to other domains. Thus, our goal in this paper differs from that in previous work. We do not aim to set a new state-of-the-art in direct speech detection, but instead: &lt;/p>
&lt;p>a) present a method that can leverage large amounts of weakly labelled data extracted from raw text, and &lt;/p>
&lt;p>b) use this model for the analysis of different distributions of direct speech across genres or time-periods. &lt;/p>
&lt;p>To the best of our knowledge, the second task has never been done on a large collection of texts.&lt;/p>
&lt;/div>
&lt;div type="div1" rend="DH-Heading1">
&lt;head>Corpus and Resources&lt;/head>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-68b34743-f5cc-960c-9591-0e3103362c5b"/>The following experiments are based on three German corpora. The first one is a large corpus containing 4600+ public domain novels including texts from the TextGrid digital library&lt;ref target="ftn1" n="1"/>and Project Gutenberg&lt;ref target="ftn2" n="2"/>. We will refer to this as the Corpus 
                    &lt;hi rend="italic">Public Domain&lt;/hi>, PD. The second one contains 800+ texts of current popular genres like romance, crime or science-fiction (Corpus 
                    &lt;hi rend="italic">Low Brow&lt;/hi>, LB). Finally, we use a corpus with 200 novels nominated for the 
                    &lt;hi rend="italic">German Book Prize&lt;/hi> or the 
                    &lt;hi rend="italic">Georg Büchner Prize&lt;/hi> (Corpus 
                    &lt;hi rend="italic">High Brow&lt;/hi>, HB).
                &lt;/p>
&lt;p>In order to train and evaluate our classifiers, we need to obtain labels specifying which parts of the texts contain direct speeches. To this end, we chose two strategies:&lt;/p>
&lt;p>For training our classifiers, we decided to extract weak labels using a simple rule based on quotation, implying everything written between quotation marks is direct speech. To yield high accuracy for this approach, it is necessary to use a well-edited collection of texts. Our PD corpus contains such a subset, which we refer to as our 
                    &lt;hi rend="italic">Kerncorpus&lt;/hi>. This 
                    &lt;hi rend="italic">Kerncorpus&lt;/hi> consists of 250 high and middle brow texts (those from the TextGrid digital library), has been manually edited and is assumed to have a mostly consistent use of quotation. 
                &lt;/p>
&lt;p>Using our quotation rule on the 
                    &lt;hi rend="italic">Kerncorpus&lt;/hi> resulted in a dataset where about 36% of tokens were marked as direct speech. In order to assess the quality of these weak labels, we gave 500 of the sentences to domain experts for manual correction. We found that there was an error-rate of about 3% in those sentences, mostly caused by nested direct speech or inscriptions being enclosed by quotation marks.
                &lt;/p>
&lt;p>For further evaluation, we chose to annotate a smaller subset of the corpus 
                    &lt;hi rend="italic">LB &lt;/hi>by hand. We selected 50 snippets from texts of low brow literature. This dataset, referred to as 
                    &lt;hi rend="italic">ALB&lt;/hi>, is relatively skewed towards text outside direct speech, with only about 18% of tokens in a direct speech.
                &lt;/p>
&lt;/div>
&lt;div type="div1" rend="DH-Heading1">
&lt;head>Experiments&lt;/head>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-68b34743-f5cd-a66a-7b59-c12c3e1ae61c"/>The following experiments use both labelled subsets described above, the large 
                    &lt;hi rend="italic">Kerncorpus&lt;ref target="ftn3" n="3"/>&lt;/hi>
                    and the smaller 
                    &lt;hi rend="italic">ALB&lt;/hi>. For all experiments, quotation marks are removed from the texts. This is done to avoid training models that rely only on the formal style of qualifying direct speech, but also consider implicit signs like the use of first person verbs or speech words.
                &lt;/p>
&lt;p>We conducted experiments on two different levels, starting with a sentence classification task, which is then refined to detect direct speech on word-level.&lt;/p>
&lt;div type="div2" rend="DH-Heading2">
&lt;head>Sentence-Level Classification&lt;/head>
&lt;p>In our first classification task, documents are split into sentences and vectorised by storing each sentence in a bag-of-words representation. To create a baseline for measuring the advantage using deep learning for direct speech recognition, we compared the performance of traditional machine learning algorithms on our labelled datasets. Training and testing some of the most common machine learning classifiers to detect sentences containing at least one word of direct speech leads to an accuracy of 
                        &lt;hi rend="bold">0.85 &lt;/hi>using Logistic Regression; for more results see Table 1.
                    &lt;/p>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-68b34743-f5ce-5a3f-4208-237d90c31d50"/>Using the same setting and replacing machine learning with a combination of recurrent and convolutional neural networks (see Chollet 2017 and Goodfellow 2017) ended up with an accuracy of 
                        &lt;hi rend="bold">0.84&lt;/hi>.
                    &lt;/p>
&lt;p>
&lt;figure>
&lt;graphic url="KONLE_Leonard_Analysing_Direct_Speech_in_German_Novels-100002010000042C00000104B331E8B55EE32983.png"/>
&lt;/figure>
&lt;/p>
&lt;p>Since we noticed that three of our classifiers all ended up with about the same score, we decided to give the task to two human annotators to establish an upper bound. We selected 250 sentences for manual annotation and again removed all quotation marks. Both annotators ended up with an accuracy comparable to that of the best machine learning methods, 84% and 82.8% respectively. From this result we concluded that it is not expedient to further optimise the sentence classification task, as we had already reached human-level accuracy.&lt;/p>
&lt;/div>
&lt;div type="div2" rend="DH-Heading2">
&lt;head>Word-Level Classification&lt;/head>
&lt;p>Because of the results from the previous section, we decided to modify our task to a word-level prediction, which enables us to include more context by ignoring sentence boundaries and at the same time make more fine-grained predictions. In this second classification task, each word is to be classified separately as inside or outside a direct speech. As baseline for this task, we trained a Linear Chain Conditional Random Field (CRF) that was only given the word itself and its part-of-speech tag. This CRF stagnated at a comparably low accuracy of 0
                        &lt;hi rend="bold">.71&lt;/hi> using cross-validation on the 
                        &lt;hi rend="italic">Kerncorpus&lt;/hi>. 
                    &lt;/p>
&lt;p>Since our goal was to provide the classifier with more context, we chose to use an architecture based on recurrent neural networks, which are able to deal with relatively large contexts. Our assumption here is that, for a good classification, we need context from both before and after the target word itself, as markers for direct speech can be found at the beginning or the end of the direct speech. We thus designed a two-branch network, visualised in Figure 1. This network receives as input a text-segment, specifically the target word in its context. The words of the input are then passed through an embedding layer and split into two parts, where the first part contains the context up to the target word and the second part contains the context following the target word. The target word itself is contained in both parts. Each part is passed through three separate LSTM-layers. In the future-branch, the context is passed through the layers in reverse, so that the target word is the last word to be read in both branches. The LSTM-layers in the past-branch are stateful and can therefore theoretically retain the entire context of the novel up to the target word. The outputs of the final LSTM-layer of both branches are concatenated. The final prediction is made based on this concatenation by a fully connected layer.&lt;/p>
&lt;p>In our best setup, we used 60 words before and after the target word as context. &lt;/p>
&lt;p>Training on one half of the 
                        &lt;hi rend="italic">Kerncorpus&lt;/hi> and evaluating on the other half, this setup yielded an accuracy of 
                        &lt;hi rend="bold">0.83&lt;/hi>. Training on the full 
                        &lt;hi rend="italic">Kerncorpus&lt;/hi> and evaluating on the manually annotated 
                        &lt;hi rend="italic">ALB&lt;/hi> reached an even better accuracy of 
                        &lt;hi rend="bold">0.90&lt;/hi>.
                        &lt;lb/>
&lt;/p>
&lt;p>
&lt;figure>
&lt;graphic url="KONLE_Leonard_Analysing_Direct_Speech_in_German_Novels-fig1.png"/&gt;
&lt;head>
&lt;lb/>Figure 1: Architecture of recurrent network to detect direct speech.
                            &lt;/head>
&lt;/figure>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;div type="div1" rend="DH-Heading1">
&lt;head>Distribution of direct speech&lt;/head>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-68b34743-f5cf-1167-4108-44c957131b2c"/>In the following experiments, we used the model based on the architecture described above. We trained this model on the 
                    &lt;hi rend="italic">Kerncorpus&lt;/hi> and used it to detect direct speech in the complete corpora 
                    &lt;hi rend="italic">PD&lt;/hi>, 
                    &lt;hi rend="italic">LB&lt;/hi> and 
                    &lt;hi rend="italic">HB&lt;/hi>. Here, we describe our findings on these corpora.
                &lt;/p>
&lt;div type="div2" rend="DH-Heading2">
&lt;head>Direct speech in 19th Century Fiction&lt;/head>
&lt;figure>
&lt;graphic url="KONLE_Leonard_Analysing_Direct_Speech_in_German_Novels-fig2.png"/>
&lt;head>
&lt;lb/>Figure 2: Ratio of direct speech in German novels from 1800 – 1900.
                                    &lt;/head>
&lt;/figure>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-68b34743-f5cf-6193-7f0a-062338eea011"/>Figure 2 shows the ratio of direct speech in German novels from 1800 till 1900 based on the texts from Corpus 
                        &lt;hi rend="italic">PD&lt;/hi>. The regression line indicates a decline of direct speech over time; at the same time, we can observe a decrease of variance. The strong variations between certain years, especially in the early 19th century, are caused by low numbers of provided texts (see Fig. 3). For instance, the peak in 1805 can be explained by the first publication of Denis Diderots ”Herrn Rameaus Neffe”, a philosophical dialogue-based novel.
                    &lt;/p>
&lt;figure>
&lt;graphic url="KONLE_Leonard_Analysing_Direct_Speech_in_German_Novels-fig3.png"/>
&lt;head>
&lt;lb/>Figure 3: Number of provided novels per year.
                                    &lt;/head>
&lt;/figure>
&lt;/div>
&lt;div type="div2" rend="DH-Heading2">
&lt;head>Distribution of direct speech in low and high brow literature&lt;/head>
&lt;figure>
&lt;graphic url="KONLE_Leonard_Analysing_Direct_Speech_in_German_Novels-fig4.png"/>
&lt;head>
&lt;lb/>Figure 4: Ratio of direct speech in German low and high brow novels after 1945.
                                    &lt;/head>
&lt;/figure>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-68b34743-f5cf-f881-8f33-e5d9f49c797b"/>There is an assumption in literary studies that a huge amount of direct speech is an indicator of low brow fiction. Figure 4 shows the ratio of direct speech between Corpus 
                        &lt;hi rend="italic">LB&lt;/hi> and 
                        &lt;hi rend="italic">HB&lt;/hi>. While the mean usage of direct speech is nearly equal in both groups, the high brow literature is far more variable.
                    &lt;/p>
&lt;p>
                        This finding is contrary to the assumption mentioned above. We propose that, while there is no clear difference in the average use of direct speech between high and low brow literature, authors in high brow literature are far more flexible in choosing how much direct speech they use in their novels. Low brow literature, on the other hand, is expected to have a rather constant amount of dialogue.
                    &lt;/p>
&lt;/div>
&lt;/div>
&lt;div type="div1" rend="DH-Heading1">
&lt;head>Conclusion and Future Work&lt;/head>
&lt;p>
&lt;anchor xml:id="id_docs-internal-guid-68b34743-f5d0-46e7-1731-31344ad11bf6"/>In this paper, we introduced a neural network architecture that is able to learn the classification of direct speech by training on weakly labelled data. This network works purely on the raw text of a novel by taking into account a relatively large context. We also demonstrate that training on weakly labelled data leads to satisfying results.
                &lt;/p>
&lt;p>While an accuracy of 0.9 is remarkable, there is still need for optimisation. Recent developments in the performance of neural networks by adding an attention mechanism (see Rush 2015) could improve the results.&lt;/p>
&lt;p>We used our neural network to analyse the distribution of direct speech over time and genres. Besides algorithmic refinements, there is a lot of potential in adding more text to our corpus and refining metadata to allow more sophisticated research questions like differences between or development of direct speech in certain genres.&lt;/p>
&lt;/div>
&lt;/body>
&lt;back>
&lt;div type="notes">
&lt;note xml:id="ftn1" n="1" place="foot">
&lt;ptr target="https://textgrid.de/digitale-bibliothek"/>
&lt;/note>
&lt;note xml:id="ftn2" n="2" place="foot">
&lt;ref target="https://gutenberg.spiegel.de/">https://gutenberg.spiegel.de&lt;/ref>
&lt;/note>
&lt;note xml:id="ftn3" n="3">
&lt;anchor xml:id="id_docs-internal-guid-5e3754e0-f67e-59e2-5068-babb8eab0c70"/> We cannot use the remaining texts for either training or evaluation, as we do not have any reliable source of labels for these texts.
            &lt;/note>
&lt;/div>
&lt;div type="bibliogr">
&lt;listBibl>
&lt;head>Bibliography&lt;/head>
&lt;bibl>
&lt;hi rend="bold">Brunner, Annelen&lt;/hi> (2013): “Automatic recognition of speech, thought, and writing representation in German narrative texts”, in 
                        &lt;hi rend="italic">Literary and Linguistic Computing. Vol. 28 (2013).&lt;/hi>
&lt;/bibl>
&lt;bibl>
&lt;hi rend="bold">Chollet, Francois&lt;/hi> (2017): “Deep Learning with Python”. Manning Publications. New York. (Preprint: https://www.manning.com/books/deep-learning-with-python)
                    &lt;/bibl>
&lt;bibl>
&lt;hi rend="bold">Goodfellow, Ian /  Bengio Yoshua / Courville,  Aaron&lt;/hi> (2016): “Deep Learning”. MIT Press. (URL: 
                        &lt;ref target="http://www.deeplearningbook.org/"&gt;
&lt;hi rend="underline">http://www.deeplearningbook.org&lt;/hi>
&lt;/ref>)
                    &lt;/bibl>
&lt;bibl>
&lt;hi rend="bold">Rush, Alexander M. / Chopra, Sumit / Weston, Jason&lt;/hi> (2015): “A Neural Attention Model for Abstractive Sentence Summarization”. 
                        &lt;hi rend="italic">arXiv preprint arXiv:1509.00685.&lt;/hi>
&lt;/bibl>
&lt;bibl>
&lt;hi rend="bold">Scheible, C., Klinger, R. &amp;amp; Padó, S.&lt;/hi> (2016): “Model Architectures for Quotation Detection”, in 
                        &lt;hi rend="italic">Proceedings of ACL (p./pp. 1736–1745).&lt;/hi>
&lt;/bibl>
&lt;/listBibl>
&lt;/div>
&lt;/back>
&lt;/text>
&lt;/TEI>
    </textSource>
    <text>


  Detecting direct speech in fiction allows gaining insight into an important element of its narrative structure. In literary studies, there are assumptions on the factors influencing the distribution of direct speech, like genre, period and aesthetic complexity.  

  

This paper aims to provide a detailed analysis of the use of direct speech across different time periods and domains. To create a reliable database for these analyses, we need to measure the usage of direct speech in a large and representative corpus. This task is more challenging than it may sound: While, nowadays, direct speech is often marked very explicitly by the use of quotes, this has not always been consistently the case. Many historical novels are not available in a well-edited form, meaning that there may be inconsistent use of quotation, or no quotation at all (Brunner, 2013). In this case, a more robust method for detecting direct speech is necessary.

  

Our first contribution is therefore a deep learning-based method to detect direct speech using large amounts of rule-based, but slightly flawed, labelled data extracted from raw text.  This has multiple advantages over the use of manually annotated training data: First, manually annotating large amounts of text is very time-intensive and therefore costly. Furthermore, annotations for one type of texts may not be transferable to other types, leading to the necessity of new annotated data for new corpora. Being able to learn from the already existing weakly labelled data is therefore desirable, as this data can automatically be extracted for a new corpus.

  

Our second contribution is the application of this approach on curated texts to gain insight in trends of direct speech distribution. On one hand we try to look for development of direct speech over time, analysing a large dataset of novels from the nineteenth century, on the other hand we focus on differences in genre comparing contemporary high and low brow literature.

  


  There have been several previous approaches to direct speech detection applying machine learning methods.  

  

For example, Brunner (2013) tests rule-based and machine learning driven classification, as well as combinations of both, on German novels. She recommends using a pure machine learning approach (Random Forest), reaching an F1 score of 0.87.

  

Scheible et al. (2016) employ a simple greedy algorithm and a semi-Markov model, showing that the latter outperforms the previous state-of-the-art by achieving a precision of 0.88.

  

Although the results seem quite satisfying, these systems require a relatively large amount of labelled data for training. As stated above, this is problematic because of the need for expensive annotation and lack of transferability to other domains. Thus, our goal in this paper differs from that in previous work. We do not aim to set a new state-of-the-art in direct speech detection, but instead: 

  

a) present a method that can leverage large amounts of weakly labelled data extracted from raw text, and 

  

b) use this model for the analysis of different distributions of direct speech across genres or time-periods. 

  

To the best of our knowledge, the second task has never been done on a large collection of texts.

  


  The following experiments are based on three German corpora. The first one is a large corpus containing 4600+ public domain novels including texts from the TextGrid digital library

and Project Gutenberg

. We will refer to this as the Corpus   Public Domain, PD. The second one contains 800+ texts of current popular genres like romance, crime or science-fiction (Corpus   Low Brow, LB). Finally, we use a corpus with 200 novels nominated for the   German Book Prize or the   Georg Büchner Prize (Corpus   High Brow, HB).  

  

In order to train and evaluate our classifiers, we need to obtain labels specifying which parts of the texts contain direct speeches. To this end, we chose two strategies:

  

For training our classifiers, we decided to extract weak labels using a simple rule based on quotation, implying everything written between quotation marks is direct speech. To yield high accuracy for this approach, it is necessary to use a well-edited collection of texts. Our PD corpus contains such a subset, which we refer to as our   Kerncorpus. This   Kerncorpus consists of 250 high and middle brow texts (those from the TextGrid digital library), has been manually edited and is assumed to have a mostly consistent use of quotation.   

  

Using our quotation rule on the   Kerncorpus resulted in a dataset where about 36% of tokens were marked as direct speech. In order to assess the quality of these weak labels, we gave 500 of the sentences to domain experts for manual correction. We found that there was an error-rate of about 3% in those sentences, mostly caused by nested direct speech or inscriptions being enclosed by quotation marks.  

  

For further evaluation, we chose to annotate a smaller subset of the corpus   LB by hand. We selected 50 snippets from texts of low brow literature. This dataset, referred to as   ALB, is relatively skewed towards text outside direct speech, with only about 18% of tokens in a direct speech.  


    

  The following experiments use both labelled subsets described above, the large   Kerncorpus

                     and the smaller   ALB. For all experiments, quotation marks are removed from the texts. This is done to avoid training models that rely only on the formal style of qualifying direct speech, but also consider implicit signs like the use of first person verbs or speech words.  

  

We conducted experiments on two different levels, starting with a sentence classification task, which is then refined to detect direct speech on word-level.

  

    

In our first classification task, documents are split into sentences and vectorised by storing each sentence in a bag-of-words representation. To create a baseline for measuring the advantage using deep learning for direct speech recognition, we compared the performance of traditional machine learning algorithms on our labelled datasets. Training and testing some of the most common machine learning classifiers to detect sentences containing at least one word of direct speech leads to an accuracy of   0.85 using Logistic Regression; for more results see Table 1.  

  

  Using the same setting and replacing machine learning with a combination of recurrent and convolutional neural networks (see Chollet 2017 and Goodfellow 2017) ended up with an accuracy of   0.84.  


  

Since we noticed that three of our classifiers all ended up with about the same score, we decided to give the task to two human annotators to establish an upper bound. We selected 250 sentences for manual annotation and again removed all quotation marks. Both annotators ended up with an accuracy comparable to that of the best machine learning methods, 84% and 82.8% respectively. From this result we concluded that it is not expedient to further optimise the sentence classification task, as we had already reached human-level accuracy.


    

Because of the results from the previous section, we decided to modify our task to a word-level prediction, which enables us to include more context by ignoring sentence boundaries and at the same time make more fine-grained predictions. In this second classification task, each word is to be classified separately as inside or outside a direct speech. As baseline for this task, we trained a Linear Chain Conditional Random Field (CRF) that was only given the word itself and its part-of-speech tag. This CRF stagnated at a comparably low accuracy of 0  .71 using cross-validation on the   Kerncorpus.   

  

Since our goal was to provide the classifier with more context, we chose to use an architecture based on recurrent neural networks, which are able to deal with relatively large contexts. Our assumption here is that, for a good classification, we need context from both before and after the target word itself, as markers for direct speech can be found at the beginning or the end of the direct speech. We thus designed a two-branch network, visualised in Figure 1. This network receives as input a text-segment, specifically the target word in its context. The words of the input are then passed through an embedding layer and split into two parts, where the first part contains the context up to the target word and the second part contains the context following the target word. The target word itself is contained in both parts. Each part is passed through three separate LSTM-layers. In the future-branch, the context is passed through the layers in reverse, so that the target word is the last word to be read in both branches. The LSTM-layers in the past-branch are stateful and can therefore theoretically retain the entire context of the novel up to the target word. The outputs of the final LSTM-layer of both branches are concatenated. The final prediction is made based on this concatenation by a fully connected layer.

  

In our best setup, we used 60 words before and after the target word as context. 

  

Training on one half of the   Kerncorpus and evaluating on the other half, this setup yielded an accuracy of   0.83. Training on the full   Kerncorpus and evaluating on the manually annotated   ALB reached an even better accuracy of   0.90.  


    

  In the following experiments, we used the model based on the architecture described above. We trained this model on the   Kerncorpus and used it to detect direct speech in the complete corpora   PD,   LB and   HB. Here, we describe our findings on these corpora.  


  Figure 2 shows the ratio of direct speech in German novels from 1800 till 1900 based on the texts from Corpus   PD. The regression line indicates a decline of direct speech over time; at the same time, we can observe a decrease of variance. The strong variations between certain years, especially in the early 19th century, are caused by low numbers of provided texts (see Fig. 3). For instance, the peak in 1805 can be explained by the first publication of Denis Diderots ”Herrn Rameaus Neffe”, a philosophical dialogue-based novel.  

    


  There is an assumption in literary studies that a huge amount of direct speech is an indicator of low brow fiction. Figure 4 shows the ratio of direct speech between Corpus   LB and   HB. While the mean usage of direct speech is nearly equal in both groups, the high brow literature is far more variable.  

  

                         This finding is contrary to the assumption mentioned above. We propose that, while there is no clear difference in the average use of direct speech between high and low brow literature, authors in high brow literature are far more flexible in choosing how much direct speech they use in their novels. Low brow literature, on the other hand, is expected to have a rather constant amount of dialogue.  

  


    

  In this paper, we introduced a neural network architecture that is able to learn the classification of direct speech by training on weakly labelled data. This network works purely on the raw text of a novel by taking into account a relatively large context. We also demonstrate that training on weakly labelled data leads to satisfying results.  

  

While an accuracy of 0.9 is remarkable, there is still need for optimisation. Recent developments in the performance of neural networks by adding an attention mechanism (see Rush 2015) could improve the results.

  

We used our neural network to analyse the distribution of direct speech over time and genres. Besides algorithmic refinements, there is a lot of potential in adding more text to our corpus and refining metadata to allow more sophisticated research questions like differences between or development of direct speech in certain genres.




Introduction  



Related Work and Task Description



Corpus and Resources



Experiments



Sentence-Level Classification



    



Word-Level Classification



  




Figure 1: Architecture of recurrent network to detect direct speech.  



Distribution of direct speech



Direct speech in 19th Century Fiction

Figure 2: Ratio of direct speech in German novels from 1800 – 1900.  



      



  
Figure 3: Number of provided novels per year.  



Distribution of direct speech in low and high brow literature



      



  
Figure 4: Ratio of direct speech in German low and high brow novels after 1945.  



Conclusion and Future Work



  


        Brunner, Annelen (2013): “Automatic recognition of speech, thought, and writing representation in German narrative texts”, in   Literary and Linguistic Computing. Vol. 28 (2013).      Chollet, Francois (2017): “Deep Learning with Python”. Manning Publications. New York. (Preprint: https://www.manning.com/books/deep-learning-with-python)      Goodfellow, Ian /  Bengio Yoshua / Courville,  Aaron (2016): “Deep Learning”. MIT Press. (URL:   
  http://www.deeplearningbook.org  
)      Rush, Alexander M. / Chopra, Sumit / Weston, Jason (2015): “A Neural Attention Model for Abstractive Sentence Summarization”.   arXiv preprint arXiv:1509.00685.      Scheible, C., Klinger, R. &amp; Padó, S. (2016): “Model Architectures for Quotation Detection”, in   Proceedings of ACL (p./pp. 1736–1745).      

  

https://gutenberg.spiegel.de
  



   We cannot use the remaining texts for either training or evaluation, as we do not have any reliable source of labels for these texts.  



Bibliography
    </text>
    <tc:tokens xmlns:tc="http://www.dspin.de/data/textcorpus">
      <tc:token ID="w1">Detecting</tc:token>
      <tc:token ID="w2">direct</tc:token>
      <tc:token ID="w3">speech</tc:token>
      <tc:token ID="w4">in</tc:token>
      <tc:token ID="w5">fiction</tc:token>
      <tc:token ID="w6">allows</tc:token>
      <tc:token ID="w7">gaining</tc:token>
      <tc:token ID="w8">insight</tc:token>
      <tc:token ID="w9">into</tc:token>
      <tc:token ID="wa">an</tc:token>
      <tc:token ID="wb">important</tc:token>
      <tc:token ID="wc">element</tc:token>
      <tc:token ID="wd">of</tc:token>
      <tc:token ID="we">its</tc:token>
      <tc:token ID="wf">narrative</tc:token>
      <tc:token ID="w10">structure</tc:token>
      <tc:token ID="w11">.</tc:token>
      <tc:token ID="w12">In</tc:token>
      <tc:token ID="w13">literary</tc:token>
      <tc:token ID="w14">studies</tc:token>
      <tc:token ID="w15">,</tc:token>
      <tc:token ID="w16">there</tc:token>
      <tc:token ID="w17">are</tc:token>
      <tc:token ID="w18">assumptions</tc:token>
      <tc:token ID="w19">on</tc:token>
      <tc:token ID="w1a">the</tc:token>
      <tc:token ID="w1b">factors</tc:token>
      <tc:token ID="w1c">influencing</tc:token>
      <tc:token ID="w1d">the</tc:token>
      <tc:token ID="w1e">distribution</tc:token>
      <tc:token ID="w1f">of</tc:token>
      <tc:token ID="w20">direct</tc:token>
      <tc:token ID="w21">speech</tc:token>
      <tc:token ID="w22">,</tc:token>
      <tc:token ID="w23">like</tc:token>
      <tc:token ID="w24">genre</tc:token>
      <tc:token ID="w25">,</tc:token>
      <tc:token ID="w26">period</tc:token>
      <tc:token ID="w27">and</tc:token>
      <tc:token ID="w28">aesthetic</tc:token>
      <tc:token ID="w29">complexity</tc:token>
      <tc:token ID="w2a">.</tc:token>
      <tc:token ID="w2b">This</tc:token>
      <tc:token ID="w2c">paper</tc:token>
      <tc:token ID="w2d">aims</tc:token>
      <tc:token ID="w2e">to</tc:token>
      <tc:token ID="w2f">provide</tc:token>
      <tc:token ID="w30">a</tc:token>
      <tc:token ID="w31">detailed</tc:token>
      <tc:token ID="w32">analysis</tc:token>
      <tc:token ID="w33">of</tc:token>
      <tc:token ID="w34">the</tc:token>
      <tc:token ID="w35">use</tc:token>
      <tc:token ID="w36">of</tc:token>
      <tc:token ID="w37">direct</tc:token>
      <tc:token ID="w38">speech</tc:token>
      <tc:token ID="w39">across</tc:token>
      <tc:token ID="w3a">different</tc:token>
      <tc:token ID="w3b">time</tc:token>
      <tc:token ID="w3c">periods</tc:token>
      <tc:token ID="w3d">and</tc:token>
      <tc:token ID="w3e">domains</tc:token>
      <tc:token ID="w3f">.</tc:token>
      <tc:token ID="w40">To</tc:token>
      <tc:token ID="w41">create</tc:token>
      <tc:token ID="w42">a</tc:token>
      <tc:token ID="w43">reliable</tc:token>
      <tc:token ID="w44">database</tc:token>
      <tc:token ID="w45">for</tc:token>
      <tc:token ID="w46">these</tc:token>
      <tc:token ID="w47">analyses</tc:token>
      <tc:token ID="w48">,</tc:token>
      <tc:token ID="w49">we</tc:token>
      <tc:token ID="w4a">need</tc:token>
      <tc:token ID="w4b">to</tc:token>
      <tc:token ID="w4c">measure</tc:token>
      <tc:token ID="w4d">the</tc:token>
      <tc:token ID="w4e">usage</tc:token>
      <tc:token ID="w4f">of</tc:token>
      <tc:token ID="w50">direct</tc:token>
      <tc:token ID="w51">speech</tc:token>
      <tc:token ID="w52">in</tc:token>
      <tc:token ID="w53">a</tc:token>
      <tc:token ID="w54">large</tc:token>
      <tc:token ID="w55">and</tc:token>
      <tc:token ID="w56">representative</tc:token>
      <tc:token ID="w57">corpus</tc:token>
      <tc:token ID="w58">.</tc:token>
      <tc:token ID="w59">This</tc:token>
      <tc:token ID="w5a">task</tc:token>
      <tc:token ID="w5b">is</tc:token>
      <tc:token ID="w5c">more</tc:token>
      <tc:token ID="w5d">challenging</tc:token>
      <tc:token ID="w5e">than</tc:token>
      <tc:token ID="w5f">it</tc:token>
      <tc:token ID="w60">may</tc:token>
      <tc:token ID="w61">sound</tc:token>
      <tc:token ID="w62">:</tc:token>
      <tc:token ID="w63">While</tc:token>
      <tc:token ID="w64">,</tc:token>
      <tc:token ID="w65">nowadays</tc:token>
      <tc:token ID="w66">,</tc:token>
      <tc:token ID="w67">direct</tc:token>
      <tc:token ID="w68">speech</tc:token>
      <tc:token ID="w69">is</tc:token>
      <tc:token ID="w6a">often</tc:token>
      <tc:token ID="w6b">marked</tc:token>
      <tc:token ID="w6c">very</tc:token>
      <tc:token ID="w6d">explicitly</tc:token>
      <tc:token ID="w6e">by</tc:token>
      <tc:token ID="w6f">the</tc:token>
      <tc:token ID="w70">use</tc:token>
      <tc:token ID="w71">of</tc:token>
      <tc:token ID="w72">quotes</tc:token>
      <tc:token ID="w73">,</tc:token>
      <tc:token ID="w74">this</tc:token>
      <tc:token ID="w75">has</tc:token>
      <tc:token ID="w76">not</tc:token>
      <tc:token ID="w77">always</tc:token>
      <tc:token ID="w78">been</tc:token>
      <tc:token ID="w79">consistently</tc:token>
      <tc:token ID="w7a">the</tc:token>
      <tc:token ID="w7b">case</tc:token>
      <tc:token ID="w7c">.</tc:token>
      <tc:token ID="w7d">Many</tc:token>
      <tc:token ID="w7e">historical</tc:token>
      <tc:token ID="w7f">novels</tc:token>
      <tc:token ID="w80">are</tc:token>
      <tc:token ID="w81">not</tc:token>
      <tc:token ID="w82">available</tc:token>
      <tc:token ID="w83">in</tc:token>
      <tc:token ID="w84">a</tc:token>
      <tc:token ID="w85">well-edited</tc:token>
      <tc:token ID="w86">form</tc:token>
      <tc:token ID="w87">,</tc:token>
      <tc:token ID="w88">meaning</tc:token>
      <tc:token ID="w89">that</tc:token>
      <tc:token ID="w8a">there</tc:token>
      <tc:token ID="w8b">may</tc:token>
      <tc:token ID="w8c">be</tc:token>
      <tc:token ID="w8d">inconsistent</tc:token>
      <tc:token ID="w8e">use</tc:token>
      <tc:token ID="w8f">of</tc:token>
      <tc:token ID="w90">quotation</tc:token>
      <tc:token ID="w91">,</tc:token>
      <tc:token ID="w92">or</tc:token>
      <tc:token ID="w93">no</tc:token>
      <tc:token ID="w94">quotation</tc:token>
      <tc:token ID="w95">at</tc:token>
      <tc:token ID="w96">all</tc:token>
      <tc:token ID="w97">(</tc:token>
      <tc:token ID="w98">Brunner</tc:token>
      <tc:token ID="w99">,</tc:token>
      <tc:token ID="w9a">2013</tc:token>
      <tc:token ID="w9b">)</tc:token>
      <tc:token ID="w9c">.</tc:token>
      <tc:token ID="w9d">In</tc:token>
      <tc:token ID="w9e">this</tc:token>
      <tc:token ID="w9f">case</tc:token>
      <tc:token ID="wa0">,</tc:token>
      <tc:token ID="wa1">a</tc:token>
      <tc:token ID="wa2">more</tc:token>
      <tc:token ID="wa3">robust</tc:token>
      <tc:token ID="wa4">method</tc:token>
      <tc:token ID="wa5">for</tc:token>
      <tc:token ID="wa6">detecting</tc:token>
      <tc:token ID="wa7">direct</tc:token>
      <tc:token ID="wa8">speech</tc:token>
      <tc:token ID="wa9">is</tc:token>
      <tc:token ID="waa">necessary</tc:token>
      <tc:token ID="wab">.</tc:token>
      <tc:token ID="wac">Our</tc:token>
      <tc:token ID="wad">first</tc:token>
      <tc:token ID="wae">contribution</tc:token>
      <tc:token ID="waf">is</tc:token>
      <tc:token ID="wb0">therefore</tc:token>
      <tc:token ID="wb1">a</tc:token>
      <tc:token ID="wb2">deep</tc:token>
      <tc:token ID="wb3">learning-based</tc:token>
      <tc:token ID="wb4">method</tc:token>
      <tc:token ID="wb5">to</tc:token>
      <tc:token ID="wb6">detect</tc:token>
      <tc:token ID="wb7">direct</tc:token>
      <tc:token ID="wb8">speech</tc:token>
      <tc:token ID="wb9">using</tc:token>
      <tc:token ID="wba">large</tc:token>
      <tc:token ID="wbb">amounts</tc:token>
      <tc:token ID="wbc">of</tc:token>
      <tc:token ID="wbd">rule-based</tc:token>
      <tc:token ID="wbe">,</tc:token>
      <tc:token ID="wbf">but</tc:token>
      <tc:token ID="wc0">slightly</tc:token>
      <tc:token ID="wc1">flawed</tc:token>
      <tc:token ID="wc2">,</tc:token>
      <tc:token ID="wc3">labelled</tc:token>
      <tc:token ID="wc4">data</tc:token>
      <tc:token ID="wc5">extracted</tc:token>
      <tc:token ID="wc6">from</tc:token>
      <tc:token ID="wc7">raw</tc:token>
      <tc:token ID="wc8">text</tc:token>
      <tc:token ID="wc9">.</tc:token>
      <tc:token ID="wca">This</tc:token>
      <tc:token ID="wcb">has</tc:token>
      <tc:token ID="wcc">multiple</tc:token>
      <tc:token ID="wcd">advantages</tc:token>
      <tc:token ID="wce">over</tc:token>
      <tc:token ID="wcf">the</tc:token>
      <tc:token ID="wd0">use</tc:token>
      <tc:token ID="wd1">of</tc:token>
      <tc:token ID="wd2">manually</tc:token>
      <tc:token ID="wd3">annotated</tc:token>
      <tc:token ID="wd4">training</tc:token>
      <tc:token ID="wd5">data</tc:token>
      <tc:token ID="wd6">:</tc:token>
      <tc:token ID="wd7">First</tc:token>
      <tc:token ID="wd8">,</tc:token>
      <tc:token ID="wd9">manually</tc:token>
      <tc:token ID="wda">annotating</tc:token>
      <tc:token ID="wdb">large</tc:token>
      <tc:token ID="wdc">amounts</tc:token>
      <tc:token ID="wdd">of</tc:token>
      <tc:token ID="wde">text</tc:token>
      <tc:token ID="wdf">is</tc:token>
      <tc:token ID="we0">very</tc:token>
      <tc:token ID="we1">time-intensive</tc:token>
      <tc:token ID="we2">and</tc:token>
      <tc:token ID="we3">therefore</tc:token>
      <tc:token ID="we4">costly</tc:token>
      <tc:token ID="we5">.</tc:token>
      <tc:token ID="we6">Furthermore</tc:token>
      <tc:token ID="we7">,</tc:token>
      <tc:token ID="we8">annotations</tc:token>
      <tc:token ID="we9">for</tc:token>
      <tc:token ID="wea">one</tc:token>
      <tc:token ID="web">type</tc:token>
      <tc:token ID="wec">of</tc:token>
      <tc:token ID="wed">texts</tc:token>
      <tc:token ID="wee">may</tc:token>
      <tc:token ID="wef">not</tc:token>
      <tc:token ID="wf0">be</tc:token>
      <tc:token ID="wf1">transferable</tc:token>
      <tc:token ID="wf2">to</tc:token>
      <tc:token ID="wf3">other</tc:token>
      <tc:token ID="wf4">types</tc:token>
      <tc:token ID="wf5">,</tc:token>
      <tc:token ID="wf6">leading</tc:token>
      <tc:token ID="wf7">to</tc:token>
      <tc:token ID="wf8">the</tc:token>
      <tc:token ID="wf9">necessity</tc:token>
      <tc:token ID="wfa">of</tc:token>
      <tc:token ID="wfb">new</tc:token>
      <tc:token ID="wfc">annotated</tc:token>
      <tc:token ID="wfd">data</tc:token>
      <tc:token ID="wfe">for</tc:token>
      <tc:token ID="wff">new</tc:token>
      <tc:token ID="w100">corpora</tc:token>
      <tc:token ID="w101">.</tc:token>
      <tc:token ID="w102">Being</tc:token>
      <tc:token ID="w103">able</tc:token>
      <tc:token ID="w104">to</tc:token>
      <tc:token ID="w105">learn</tc:token>
      <tc:token ID="w106">from</tc:token>
      <tc:token ID="w107">the</tc:token>
      <tc:token ID="w108">already</tc:token>
      <tc:token ID="w109">existing</tc:token>
      <tc:token ID="w10a">weakly</tc:token>
      <tc:token ID="w10b">labelled</tc:token>
      <tc:token ID="w10c">data</tc:token>
      <tc:token ID="w10d">is</tc:token>
      <tc:token ID="w10e">therefore</tc:token>
      <tc:token ID="w10f">desirable</tc:token>
      <tc:token ID="w110">,</tc:token>
      <tc:token ID="w111">as</tc:token>
      <tc:token ID="w112">this</tc:token>
      <tc:token ID="w113">data</tc:token>
      <tc:token ID="w114">can</tc:token>
      <tc:token ID="w115">automatically</tc:token>
      <tc:token ID="w116">be</tc:token>
      <tc:token ID="w117">extracted</tc:token>
      <tc:token ID="w118">for</tc:token>
      <tc:token ID="w119">a</tc:token>
      <tc:token ID="w11a">new</tc:token>
      <tc:token ID="w11b">corpus</tc:token>
      <tc:token ID="w11c">.</tc:token>
      <tc:token ID="w11d">Our</tc:token>
      <tc:token ID="w11e">second</tc:token>
      <tc:token ID="w11f">contribution</tc:token>
      <tc:token ID="w120">is</tc:token>
      <tc:token ID="w121">the</tc:token>
      <tc:token ID="w122">application</tc:token>
      <tc:token ID="w123">of</tc:token>
      <tc:token ID="w124">this</tc:token>
      <tc:token ID="w125">approach</tc:token>
      <tc:token ID="w126">on</tc:token>
      <tc:token ID="w127">curated</tc:token>
      <tc:token ID="w128">texts</tc:token>
      <tc:token ID="w129">to</tc:token>
      <tc:token ID="w12a">gain</tc:token>
      <tc:token ID="w12b">insight</tc:token>
      <tc:token ID="w12c">in</tc:token>
      <tc:token ID="w12d">trends</tc:token>
      <tc:token ID="w12e">of</tc:token>
      <tc:token ID="w12f">direct</tc:token>
      <tc:token ID="w130">speech</tc:token>
      <tc:token ID="w131">distribution</tc:token>
      <tc:token ID="w132">.</tc:token>
      <tc:token ID="w133">On</tc:token>
      <tc:token ID="w134">one</tc:token>
      <tc:token ID="w135">hand</tc:token>
      <tc:token ID="w136">we</tc:token>
      <tc:token ID="w137">try</tc:token>
      <tc:token ID="w138">to</tc:token>
      <tc:token ID="w139">look</tc:token>
      <tc:token ID="w13a">for</tc:token>
      <tc:token ID="w13b">development</tc:token>
      <tc:token ID="w13c">of</tc:token>
      <tc:token ID="w13d">direct</tc:token>
      <tc:token ID="w13e">speech</tc:token>
      <tc:token ID="w13f">over</tc:token>
      <tc:token ID="w140">time</tc:token>
      <tc:token ID="w141">,</tc:token>
      <tc:token ID="w142">analysing</tc:token>
      <tc:token ID="w143">a</tc:token>
      <tc:token ID="w144">large</tc:token>
      <tc:token ID="w145">dataset</tc:token>
      <tc:token ID="w146">of</tc:token>
      <tc:token ID="w147">novels</tc:token>
      <tc:token ID="w148">from</tc:token>
      <tc:token ID="w149">the</tc:token>
      <tc:token ID="w14a">nineteenth</tc:token>
      <tc:token ID="w14b">century</tc:token>
      <tc:token ID="w14c">,</tc:token>
      <tc:token ID="w14d">on</tc:token>
      <tc:token ID="w14e">the</tc:token>
      <tc:token ID="w14f">other</tc:token>
      <tc:token ID="w150">hand</tc:token>
      <tc:token ID="w151">we</tc:token>
      <tc:token ID="w152">focus</tc:token>
      <tc:token ID="w153">on</tc:token>
      <tc:token ID="w154">differences</tc:token>
      <tc:token ID="w155">in</tc:token>
      <tc:token ID="w156">genre</tc:token>
      <tc:token ID="w157">comparing</tc:token>
      <tc:token ID="w158">contemporary</tc:token>
      <tc:token ID="w159">high</tc:token>
      <tc:token ID="w15a">and</tc:token>
      <tc:token ID="w15b">low</tc:token>
      <tc:token ID="w15c">brow</tc:token>
      <tc:token ID="w15d">literature</tc:token>
      <tc:token ID="w15e">.</tc:token>
      <tc:token ID="w15f">There</tc:token>
      <tc:token ID="w160">have</tc:token>
      <tc:token ID="w161">been</tc:token>
      <tc:token ID="w162">several</tc:token>
      <tc:token ID="w163">previous</tc:token>
      <tc:token ID="w164">approaches</tc:token>
      <tc:token ID="w165">to</tc:token>
      <tc:token ID="w166">direct</tc:token>
      <tc:token ID="w167">speech</tc:token>
      <tc:token ID="w168">detection</tc:token>
      <tc:token ID="w169">applying</tc:token>
      <tc:token ID="w16a">machine</tc:token>
      <tc:token ID="w16b">learning</tc:token>
      <tc:token ID="w16c">methods</tc:token>
      <tc:token ID="w16d">.</tc:token>
      <tc:token ID="w16e">For</tc:token>
      <tc:token ID="w16f">example</tc:token>
      <tc:token ID="w170">,</tc:token>
      <tc:token ID="w171">Brunner</tc:token>
      <tc:token ID="w172">(</tc:token>
      <tc:token ID="w173">2013</tc:token>
      <tc:token ID="w174">)</tc:token>
      <tc:token ID="w175">tests</tc:token>
      <tc:token ID="w176">rule-based</tc:token>
      <tc:token ID="w177">and</tc:token>
      <tc:token ID="w178">machine</tc:token>
      <tc:token ID="w179">learning</tc:token>
      <tc:token ID="w17a">driven</tc:token>
      <tc:token ID="w17b">classification</tc:token>
      <tc:token ID="w17c">,</tc:token>
      <tc:token ID="w17d">as</tc:token>
      <tc:token ID="w17e">well</tc:token>
      <tc:token ID="w17f">as</tc:token>
      <tc:token ID="w180">combinations</tc:token>
      <tc:token ID="w181">of</tc:token>
      <tc:token ID="w182">both</tc:token>
      <tc:token ID="w183">,</tc:token>
      <tc:token ID="w184">on</tc:token>
      <tc:token ID="w185">German</tc:token>
      <tc:token ID="w186">novels</tc:token>
      <tc:token ID="w187">.</tc:token>
      <tc:token ID="w188">She</tc:token>
      <tc:token ID="w189">recommends</tc:token>
      <tc:token ID="w18a">using</tc:token>
      <tc:token ID="w18b">a</tc:token>
      <tc:token ID="w18c">pure</tc:token>
      <tc:token ID="w18d">machine</tc:token>
      <tc:token ID="w18e">learning</tc:token>
      <tc:token ID="w18f">approach</tc:token>
      <tc:token ID="w190">(</tc:token>
      <tc:token ID="w191">Random</tc:token>
      <tc:token ID="w192">Forest</tc:token>
      <tc:token ID="w193">)</tc:token>
      <tc:token ID="w194">,</tc:token>
      <tc:token ID="w195">reaching</tc:token>
      <tc:token ID="w196">an</tc:token>
      <tc:token ID="w197">F1</tc:token>
      <tc:token ID="w198">score</tc:token>
      <tc:token ID="w199">of</tc:token>
      <tc:token ID="w19a">0.87</tc:token>
      <tc:token ID="w19b">.</tc:token>
      <tc:token ID="w19c">Scheible</tc:token>
      <tc:token ID="w19d">et</tc:token>
      <tc:token ID="w19e">al.</tc:token>
      <tc:token ID="w19f">(</tc:token>
      <tc:token ID="w1a0">2016</tc:token>
      <tc:token ID="w1a1">)</tc:token>
      <tc:token ID="w1a2">employ</tc:token>
      <tc:token ID="w1a3">a</tc:token>
      <tc:token ID="w1a4">simple</tc:token>
      <tc:token ID="w1a5">greedy</tc:token>
      <tc:token ID="w1a6">algorithm</tc:token>
      <tc:token ID="w1a7">and</tc:token>
      <tc:token ID="w1a8">a</tc:token>
      <tc:token ID="w1a9">semi-Markov</tc:token>
      <tc:token ID="w1aa">model</tc:token>
      <tc:token ID="w1ab">,</tc:token>
      <tc:token ID="w1ac">showing</tc:token>
      <tc:token ID="w1ad">that</tc:token>
      <tc:token ID="w1ae">the</tc:token>
      <tc:token ID="w1af">latter</tc:token>
      <tc:token ID="w1b0">outperforms</tc:token>
      <tc:token ID="w1b1">the</tc:token>
      <tc:token ID="w1b2">previous</tc:token>
      <tc:token ID="w1b3">state-of-the-art</tc:token>
      <tc:token ID="w1b4">by</tc:token>
      <tc:token ID="w1b5">achieving</tc:token>
      <tc:token ID="w1b6">a</tc:token>
      <tc:token ID="w1b7">precision</tc:token>
      <tc:token ID="w1b8">of</tc:token>
      <tc:token ID="w1b9">0.88</tc:token>
      <tc:token ID="w1ba">.</tc:token>
      <tc:token ID="w1bb">Although</tc:token>
      <tc:token ID="w1bc">the</tc:token>
      <tc:token ID="w1bd">results</tc:token>
      <tc:token ID="w1be">seem</tc:token>
      <tc:token ID="w1bf">quite</tc:token>
      <tc:token ID="w1c0">satisfying</tc:token>
      <tc:token ID="w1c1">,</tc:token>
      <tc:token ID="w1c2">these</tc:token>
      <tc:token ID="w1c3">systems</tc:token>
      <tc:token ID="w1c4">require</tc:token>
      <tc:token ID="w1c5">a</tc:token>
      <tc:token ID="w1c6">relatively</tc:token>
      <tc:token ID="w1c7">large</tc:token>
      <tc:token ID="w1c8">amount</tc:token>
      <tc:token ID="w1c9">of</tc:token>
      <tc:token ID="w1ca">labelled</tc:token>
      <tc:token ID="w1cb">data</tc:token>
      <tc:token ID="w1cc">for</tc:token>
      <tc:token ID="w1cd">training</tc:token>
      <tc:token ID="w1ce">.</tc:token>
      <tc:token ID="w1cf">As</tc:token>
      <tc:token ID="w1d0">stated</tc:token>
      <tc:token ID="w1d1">above</tc:token>
      <tc:token ID="w1d2">,</tc:token>
      <tc:token ID="w1d3">this</tc:token>
      <tc:token ID="w1d4">is</tc:token>
      <tc:token ID="w1d5">problematic</tc:token>
      <tc:token ID="w1d6">because</tc:token>
      <tc:token ID="w1d7">of</tc:token>
      <tc:token ID="w1d8">the</tc:token>
      <tc:token ID="w1d9">need</tc:token>
      <tc:token ID="w1da">for</tc:token>
      <tc:token ID="w1db">expensive</tc:token>
      <tc:token ID="w1dc">annotation</tc:token>
      <tc:token ID="w1dd">and</tc:token>
      <tc:token ID="w1de">lack</tc:token>
      <tc:token ID="w1df">of</tc:token>
      <tc:token ID="w1e0">transferability</tc:token>
      <tc:token ID="w1e1">to</tc:token>
      <tc:token ID="w1e2">other</tc:token>
      <tc:token ID="w1e3">domains</tc:token>
      <tc:token ID="w1e4">.</tc:token>
      <tc:token ID="w1e5">Thus</tc:token>
      <tc:token ID="w1e6">,</tc:token>
      <tc:token ID="w1e7">our</tc:token>
      <tc:token ID="w1e8">goal</tc:token>
      <tc:token ID="w1e9">in</tc:token>
      <tc:token ID="w1ea">this</tc:token>
      <tc:token ID="w1eb">paper</tc:token>
      <tc:token ID="w1ec">differs</tc:token>
      <tc:token ID="w1ed">from</tc:token>
      <tc:token ID="w1ee">that</tc:token>
      <tc:token ID="w1ef">in</tc:token>
      <tc:token ID="w1f0">previous</tc:token>
      <tc:token ID="w1f1">work</tc:token>
      <tc:token ID="w1f2">.</tc:token>
      <tc:token ID="w1f3">We</tc:token>
      <tc:token ID="w1f4">do</tc:token>
      <tc:token ID="w1f5">not</tc:token>
      <tc:token ID="w1f6">aim</tc:token>
      <tc:token ID="w1f7">to</tc:token>
      <tc:token ID="w1f8">set</tc:token>
      <tc:token ID="w1f9">a</tc:token>
      <tc:token ID="w1fa">new</tc:token>
      <tc:token ID="w1fb">state-of-the-art</tc:token>
      <tc:token ID="w1fc">in</tc:token>
      <tc:token ID="w1fd">direct</tc:token>
      <tc:token ID="w1fe">speech</tc:token>
      <tc:token ID="w1ff">detection</tc:token>
      <tc:token ID="w200">,</tc:token>
      <tc:token ID="w201">but</tc:token>
      <tc:token ID="w202">instead</tc:token>
      <tc:token ID="w203">:</tc:token>
      <tc:token ID="w204">a</tc:token>
      <tc:token ID="w205">)</tc:token>
      <tc:token ID="w206">present</tc:token>
      <tc:token ID="w207">a</tc:token>
      <tc:token ID="w208">method</tc:token>
      <tc:token ID="w209">that</tc:token>
      <tc:token ID="w20a">can</tc:token>
      <tc:token ID="w20b">leverage</tc:token>
      <tc:token ID="w20c">large</tc:token>
      <tc:token ID="w20d">amounts</tc:token>
      <tc:token ID="w20e">of</tc:token>
      <tc:token ID="w20f">weakly</tc:token>
      <tc:token ID="w210">labelled</tc:token>
      <tc:token ID="w211">data</tc:token>
      <tc:token ID="w212">extracted</tc:token>
      <tc:token ID="w213">from</tc:token>
      <tc:token ID="w214">raw</tc:token>
      <tc:token ID="w215">text</tc:token>
      <tc:token ID="w216">,</tc:token>
      <tc:token ID="w217">and</tc:token>
      <tc:token ID="w218">b</tc:token>
      <tc:token ID="w219">)</tc:token>
      <tc:token ID="w21a">use</tc:token>
      <tc:token ID="w21b">this</tc:token>
      <tc:token ID="w21c">model</tc:token>
      <tc:token ID="w21d">for</tc:token>
      <tc:token ID="w21e">the</tc:token>
      <tc:token ID="w21f">analysis</tc:token>
      <tc:token ID="w220">of</tc:token>
      <tc:token ID="w221">different</tc:token>
      <tc:token ID="w222">distributions</tc:token>
      <tc:token ID="w223">of</tc:token>
      <tc:token ID="w224">direct</tc:token>
      <tc:token ID="w225">speech</tc:token>
      <tc:token ID="w226">across</tc:token>
      <tc:token ID="w227">genres</tc:token>
      <tc:token ID="w228">or</tc:token>
      <tc:token ID="w229">time-periods</tc:token>
      <tc:token ID="w22a">.</tc:token>
      <tc:token ID="w22b">To</tc:token>
      <tc:token ID="w22c">the</tc:token>
      <tc:token ID="w22d">best</tc:token>
      <tc:token ID="w22e">of</tc:token>
      <tc:token ID="w22f">our</tc:token>
      <tc:token ID="w230">knowledge</tc:token>
      <tc:token ID="w231">,</tc:token>
      <tc:token ID="w232">the</tc:token>
      <tc:token ID="w233">second</tc:token>
      <tc:token ID="w234">task</tc:token>
      <tc:token ID="w235">has</tc:token>
      <tc:token ID="w236">never</tc:token>
      <tc:token ID="w237">been</tc:token>
      <tc:token ID="w238">done</tc:token>
      <tc:token ID="w239">on</tc:token>
      <tc:token ID="w23a">a</tc:token>
      <tc:token ID="w23b">large</tc:token>
      <tc:token ID="w23c">collection</tc:token>
      <tc:token ID="w23d">of</tc:token>
      <tc:token ID="w23e">texts</tc:token>
      <tc:token ID="w23f">.</tc:token>
      <tc:token ID="w240">The</tc:token>
      <tc:token ID="w241">following</tc:token>
      <tc:token ID="w242">experiments</tc:token>
      <tc:token ID="w243">are</tc:token>
      <tc:token ID="w244">based</tc:token>
      <tc:token ID="w245">on</tc:token>
      <tc:token ID="w246">three</tc:token>
      <tc:token ID="w247">German</tc:token>
      <tc:token ID="w248">corpora</tc:token>
      <tc:token ID="w249">.</tc:token>
      <tc:token ID="w24a">The</tc:token>
      <tc:token ID="w24b">first</tc:token>
      <tc:token ID="w24c">one</tc:token>
      <tc:token ID="w24d">is</tc:token>
      <tc:token ID="w24e">a</tc:token>
      <tc:token ID="w24f">large</tc:token>
      <tc:token ID="w250">corpus</tc:token>
      <tc:token ID="w251">containing</tc:token>
      <tc:token ID="w252">4600+</tc:token>
      <tc:token ID="w253">public</tc:token>
      <tc:token ID="w254">domain</tc:token>
      <tc:token ID="w255">novels</tc:token>
      <tc:token ID="w256">including</tc:token>
      <tc:token ID="w257">texts</tc:token>
      <tc:token ID="w258">from</tc:token>
      <tc:token ID="w259">the</tc:token>
      <tc:token ID="w25a">TextGrid</tc:token>
      <tc:token ID="w25b">digital</tc:token>
      <tc:token ID="w25c">library</tc:token>
      <tc:token ID="w25d">and</tc:token>
      <tc:token ID="w25e">Project</tc:token>
      <tc:token ID="w25f">Gutenberg</tc:token>
      <tc:token ID="w260">.</tc:token>
      <tc:token ID="w261">We</tc:token>
      <tc:token ID="w262">will</tc:token>
      <tc:token ID="w263">refer</tc:token>
      <tc:token ID="w264">to</tc:token>
      <tc:token ID="w265">this</tc:token>
      <tc:token ID="w266">as</tc:token>
      <tc:token ID="w267">the</tc:token>
      <tc:token ID="w268">Corpus</tc:token>
      <tc:token ID="w269">Public</tc:token>
      <tc:token ID="w26a">Domain</tc:token>
      <tc:token ID="w26b">,</tc:token>
      <tc:token ID="w26c">PD</tc:token>
      <tc:token ID="w26d">.</tc:token>
      <tc:token ID="w26e">The</tc:token>
      <tc:token ID="w26f">second</tc:token>
      <tc:token ID="w270">one</tc:token>
      <tc:token ID="w271">contains</tc:token>
      <tc:token ID="w272">800+</tc:token>
      <tc:token ID="w273">texts</tc:token>
      <tc:token ID="w274">of</tc:token>
      <tc:token ID="w275">current</tc:token>
      <tc:token ID="w276">popular</tc:token>
      <tc:token ID="w277">genres</tc:token>
      <tc:token ID="w278">like</tc:token>
      <tc:token ID="w279">romance</tc:token>
      <tc:token ID="w27a">,</tc:token>
      <tc:token ID="w27b">crime</tc:token>
      <tc:token ID="w27c">or</tc:token>
      <tc:token ID="w27d">science-fiction</tc:token>
      <tc:token ID="w27e">(</tc:token>
      <tc:token ID="w27f">Corpus</tc:token>
      <tc:token ID="w280">Low</tc:token>
      <tc:token ID="w281">Brow</tc:token>
      <tc:token ID="w282">,</tc:token>
      <tc:token ID="w283">LB</tc:token>
      <tc:token ID="w284">)</tc:token>
      <tc:token ID="w285">.</tc:token>
      <tc:token ID="w286">Finally</tc:token>
      <tc:token ID="w287">,</tc:token>
      <tc:token ID="w288">we</tc:token>
      <tc:token ID="w289">use</tc:token>
      <tc:token ID="w28a">a</tc:token>
      <tc:token ID="w28b">corpus</tc:token>
      <tc:token ID="w28c">with</tc:token>
      <tc:token ID="w28d">200</tc:token>
      <tc:token ID="w28e">novels</tc:token>
      <tc:token ID="w28f">nominated</tc:token>
      <tc:token ID="w290">for</tc:token>
      <tc:token ID="w291">the</tc:token>
      <tc:token ID="w292">German</tc:token>
      <tc:token ID="w293">Book</tc:token>
      <tc:token ID="w294">Prize</tc:token>
      <tc:token ID="w295">or</tc:token>
      <tc:token ID="w296">the</tc:token>
      <tc:token ID="w297">Georg</tc:token>
      <tc:token ID="w298">Büchner</tc:token>
      <tc:token ID="w299">Prize</tc:token>
      <tc:token ID="w29a">(</tc:token>
      <tc:token ID="w29b">Corpus</tc:token>
      <tc:token ID="w29c">High</tc:token>
      <tc:token ID="w29d">Brow</tc:token>
      <tc:token ID="w29e">,</tc:token>
      <tc:token ID="w29f">HB</tc:token>
      <tc:token ID="w2a0">)</tc:token>
      <tc:token ID="w2a1">.</tc:token>
      <tc:token ID="w2a2">In</tc:token>
      <tc:token ID="w2a3">order</tc:token>
      <tc:token ID="w2a4">to</tc:token>
      <tc:token ID="w2a5">train</tc:token>
      <tc:token ID="w2a6">and</tc:token>
      <tc:token ID="w2a7">evaluate</tc:token>
      <tc:token ID="w2a8">our</tc:token>
      <tc:token ID="w2a9">classifiers</tc:token>
      <tc:token ID="w2aa">,</tc:token>
      <tc:token ID="w2ab">we</tc:token>
      <tc:token ID="w2ac">need</tc:token>
      <tc:token ID="w2ad">to</tc:token>
      <tc:token ID="w2ae">obtain</tc:token>
      <tc:token ID="w2af">labels</tc:token>
      <tc:token ID="w2b0">specifying</tc:token>
      <tc:token ID="w2b1">which</tc:token>
      <tc:token ID="w2b2">parts</tc:token>
      <tc:token ID="w2b3">of</tc:token>
      <tc:token ID="w2b4">the</tc:token>
      <tc:token ID="w2b5">texts</tc:token>
      <tc:token ID="w2b6">contain</tc:token>
      <tc:token ID="w2b7">direct</tc:token>
      <tc:token ID="w2b8">speeches</tc:token>
      <tc:token ID="w2b9">.</tc:token>
      <tc:token ID="w2ba">To</tc:token>
      <tc:token ID="w2bb">this</tc:token>
      <tc:token ID="w2bc">end</tc:token>
      <tc:token ID="w2bd">,</tc:token>
      <tc:token ID="w2be">we</tc:token>
      <tc:token ID="w2bf">chose</tc:token>
      <tc:token ID="w2c0">two</tc:token>
      <tc:token ID="w2c1">strategies</tc:token>
      <tc:token ID="w2c2">:</tc:token>
      <tc:token ID="w2c3">For</tc:token>
      <tc:token ID="w2c4">training</tc:token>
      <tc:token ID="w2c5">our</tc:token>
      <tc:token ID="w2c6">classifiers</tc:token>
      <tc:token ID="w2c7">,</tc:token>
      <tc:token ID="w2c8">we</tc:token>
      <tc:token ID="w2c9">decided</tc:token>
      <tc:token ID="w2ca">to</tc:token>
      <tc:token ID="w2cb">extract</tc:token>
      <tc:token ID="w2cc">weak</tc:token>
      <tc:token ID="w2cd">labels</tc:token>
      <tc:token ID="w2ce">using</tc:token>
      <tc:token ID="w2cf">a</tc:token>
      <tc:token ID="w2d0">simple</tc:token>
      <tc:token ID="w2d1">rule</tc:token>
      <tc:token ID="w2d2">based</tc:token>
      <tc:token ID="w2d3">on</tc:token>
      <tc:token ID="w2d4">quotation</tc:token>
      <tc:token ID="w2d5">,</tc:token>
      <tc:token ID="w2d6">implying</tc:token>
      <tc:token ID="w2d7">everything</tc:token>
      <tc:token ID="w2d8">written</tc:token>
      <tc:token ID="w2d9">between</tc:token>
      <tc:token ID="w2da">quotation</tc:token>
      <tc:token ID="w2db">marks</tc:token>
      <tc:token ID="w2dc">is</tc:token>
      <tc:token ID="w2dd">direct</tc:token>
      <tc:token ID="w2de">speech</tc:token>
      <tc:token ID="w2df">.</tc:token>
      <tc:token ID="w2e0">To</tc:token>
      <tc:token ID="w2e1">yield</tc:token>
      <tc:token ID="w2e2">high</tc:token>
      <tc:token ID="w2e3">accuracy</tc:token>
      <tc:token ID="w2e4">for</tc:token>
      <tc:token ID="w2e5">this</tc:token>
      <tc:token ID="w2e6">approach</tc:token>
      <tc:token ID="w2e7">,</tc:token>
      <tc:token ID="w2e8">it</tc:token>
      <tc:token ID="w2e9">is</tc:token>
      <tc:token ID="w2ea">necessary</tc:token>
      <tc:token ID="w2eb">to</tc:token>
      <tc:token ID="w2ec">use</tc:token>
      <tc:token ID="w2ed">a</tc:token>
      <tc:token ID="w2ee">well-edited</tc:token>
      <tc:token ID="w2ef">collection</tc:token>
      <tc:token ID="w2f0">of</tc:token>
      <tc:token ID="w2f1">texts</tc:token>
      <tc:token ID="w2f2">.</tc:token>
      <tc:token ID="w2f3">Our</tc:token>
      <tc:token ID="w2f4">PD</tc:token>
      <tc:token ID="w2f5">corpus</tc:token>
      <tc:token ID="w2f6">contains</tc:token>
      <tc:token ID="w2f7">such</tc:token>
      <tc:token ID="w2f8">a</tc:token>
      <tc:token ID="w2f9">subset</tc:token>
      <tc:token ID="w2fa">,</tc:token>
      <tc:token ID="w2fb">which</tc:token>
      <tc:token ID="w2fc">we</tc:token>
      <tc:token ID="w2fd">refer</tc:token>
      <tc:token ID="w2fe">to</tc:token>
      <tc:token ID="w2ff">as</tc:token>
      <tc:token ID="w300">our</tc:token>
      <tc:token ID="w301">Kerncorpus</tc:token>
      <tc:token ID="w302">.</tc:token>
      <tc:token ID="w303">This</tc:token>
      <tc:token ID="w304">Kerncorpus</tc:token>
      <tc:token ID="w305">consists</tc:token>
      <tc:token ID="w306">of</tc:token>
      <tc:token ID="w307">250</tc:token>
      <tc:token ID="w308">high</tc:token>
      <tc:token ID="w309">and</tc:token>
      <tc:token ID="w30a">middle</tc:token>
      <tc:token ID="w30b">brow</tc:token>
      <tc:token ID="w30c">texts</tc:token>
      <tc:token ID="w30d">(</tc:token>
      <tc:token ID="w30e">those</tc:token>
      <tc:token ID="w30f">from</tc:token>
      <tc:token ID="w310">the</tc:token>
      <tc:token ID="w311">TextGrid</tc:token>
      <tc:token ID="w312">digital</tc:token>
      <tc:token ID="w313">library</tc:token>
      <tc:token ID="w314">)</tc:token>
      <tc:token ID="w315">,</tc:token>
      <tc:token ID="w316">has</tc:token>
      <tc:token ID="w317">been</tc:token>
      <tc:token ID="w318">manually</tc:token>
      <tc:token ID="w319">edited</tc:token>
      <tc:token ID="w31a">and</tc:token>
      <tc:token ID="w31b">is</tc:token>
      <tc:token ID="w31c">assumed</tc:token>
      <tc:token ID="w31d">to</tc:token>
      <tc:token ID="w31e">have</tc:token>
      <tc:token ID="w31f">a</tc:token>
      <tc:token ID="w320">mostly</tc:token>
      <tc:token ID="w321">consistent</tc:token>
      <tc:token ID="w322">use</tc:token>
      <tc:token ID="w323">of</tc:token>
      <tc:token ID="w324">quotation</tc:token>
      <tc:token ID="w325">.</tc:token>
      <tc:token ID="w326">Using</tc:token>
      <tc:token ID="w327">our</tc:token>
      <tc:token ID="w328">quotation</tc:token>
      <tc:token ID="w329">rule</tc:token>
      <tc:token ID="w32a">on</tc:token>
      <tc:token ID="w32b">the</tc:token>
      <tc:token ID="w32c">Kerncorpus</tc:token>
      <tc:token ID="w32d">resulted</tc:token>
      <tc:token ID="w32e">in</tc:token>
      <tc:token ID="w32f">a</tc:token>
      <tc:token ID="w330">dataset</tc:token>
      <tc:token ID="w331">where</tc:token>
      <tc:token ID="w332">about</tc:token>
      <tc:token ID="w333">36</tc:token>
      <tc:token ID="w334">%</tc:token>
      <tc:token ID="w335">of</tc:token>
      <tc:token ID="w336">tokens</tc:token>
      <tc:token ID="w337">were</tc:token>
      <tc:token ID="w338">marked</tc:token>
      <tc:token ID="w339">as</tc:token>
      <tc:token ID="w33a">direct</tc:token>
      <tc:token ID="w33b">speech</tc:token>
      <tc:token ID="w33c">.</tc:token>
      <tc:token ID="w33d">In</tc:token>
      <tc:token ID="w33e">order</tc:token>
      <tc:token ID="w33f">to</tc:token>
      <tc:token ID="w340">assess</tc:token>
      <tc:token ID="w341">the</tc:token>
      <tc:token ID="w342">quality</tc:token>
      <tc:token ID="w343">of</tc:token>
      <tc:token ID="w344">these</tc:token>
      <tc:token ID="w345">weak</tc:token>
      <tc:token ID="w346">labels</tc:token>
      <tc:token ID="w347">,</tc:token>
      <tc:token ID="w348">we</tc:token>
      <tc:token ID="w349">gave</tc:token>
      <tc:token ID="w34a">500</tc:token>
      <tc:token ID="w34b">of</tc:token>
      <tc:token ID="w34c">the</tc:token>
      <tc:token ID="w34d">sentences</tc:token>
      <tc:token ID="w34e">to</tc:token>
      <tc:token ID="w34f">domain</tc:token>
      <tc:token ID="w350">experts</tc:token>
      <tc:token ID="w351">for</tc:token>
      <tc:token ID="w352">manual</tc:token>
      <tc:token ID="w353">correction</tc:token>
      <tc:token ID="w354">.</tc:token>
      <tc:token ID="w355">We</tc:token>
      <tc:token ID="w356">found</tc:token>
      <tc:token ID="w357">that</tc:token>
      <tc:token ID="w358">there</tc:token>
      <tc:token ID="w359">was</tc:token>
      <tc:token ID="w35a">an</tc:token>
      <tc:token ID="w35b">error-rate</tc:token>
      <tc:token ID="w35c">of</tc:token>
      <tc:token ID="w35d">about</tc:token>
      <tc:token ID="w35e">3</tc:token>
      <tc:token ID="w35f">%</tc:token>
      <tc:token ID="w360">in</tc:token>
      <tc:token ID="w361">those</tc:token>
      <tc:token ID="w362">sentences</tc:token>
      <tc:token ID="w363">,</tc:token>
      <tc:token ID="w364">mostly</tc:token>
      <tc:token ID="w365">caused</tc:token>
      <tc:token ID="w366">by</tc:token>
      <tc:token ID="w367">nested</tc:token>
      <tc:token ID="w368">direct</tc:token>
      <tc:token ID="w369">speech</tc:token>
      <tc:token ID="w36a">or</tc:token>
      <tc:token ID="w36b">inscriptions</tc:token>
      <tc:token ID="w36c">being</tc:token>
      <tc:token ID="w36d">enclosed</tc:token>
      <tc:token ID="w36e">by</tc:token>
      <tc:token ID="w36f">quotation</tc:token>
      <tc:token ID="w370">marks</tc:token>
      <tc:token ID="w371">.</tc:token>
      <tc:token ID="w372">For</tc:token>
      <tc:token ID="w373">further</tc:token>
      <tc:token ID="w374">evaluation</tc:token>
      <tc:token ID="w375">,</tc:token>
      <tc:token ID="w376">we</tc:token>
      <tc:token ID="w377">chose</tc:token>
      <tc:token ID="w378">to</tc:token>
      <tc:token ID="w379">annotate</tc:token>
      <tc:token ID="w37a">a</tc:token>
      <tc:token ID="w37b">smaller</tc:token>
      <tc:token ID="w37c">subset</tc:token>
      <tc:token ID="w37d">of</tc:token>
      <tc:token ID="w37e">the</tc:token>
      <tc:token ID="w37f">corpus</tc:token>
      <tc:token ID="w380">LB</tc:token>
      <tc:token ID="w381">by</tc:token>
      <tc:token ID="w382">hand</tc:token>
      <tc:token ID="w383">.</tc:token>
      <tc:token ID="w384">We</tc:token>
      <tc:token ID="w385">selected</tc:token>
      <tc:token ID="w386">50</tc:token>
      <tc:token ID="w387">snippets</tc:token>
      <tc:token ID="w388">from</tc:token>
      <tc:token ID="w389">texts</tc:token>
      <tc:token ID="w38a">of</tc:token>
      <tc:token ID="w38b">low</tc:token>
      <tc:token ID="w38c">brow</tc:token>
      <tc:token ID="w38d">literature</tc:token>
      <tc:token ID="w38e">.</tc:token>
      <tc:token ID="w38f">This</tc:token>
      <tc:token ID="w390">dataset</tc:token>
      <tc:token ID="w391">,</tc:token>
      <tc:token ID="w392">referred</tc:token>
      <tc:token ID="w393">to</tc:token>
      <tc:token ID="w394">as</tc:token>
      <tc:token ID="w395">ALB</tc:token>
      <tc:token ID="w396">,</tc:token>
      <tc:token ID="w397">is</tc:token>
      <tc:token ID="w398">relatively</tc:token>
      <tc:token ID="w399">skewed</tc:token>
      <tc:token ID="w39a">towards</tc:token>
      <tc:token ID="w39b">text</tc:token>
      <tc:token ID="w39c">outside</tc:token>
      <tc:token ID="w39d">direct</tc:token>
      <tc:token ID="w39e">speech</tc:token>
      <tc:token ID="w39f">,</tc:token>
      <tc:token ID="w3a0">with</tc:token>
      <tc:token ID="w3a1">only</tc:token>
      <tc:token ID="w3a2">about</tc:token>
      <tc:token ID="w3a3">18</tc:token>
      <tc:token ID="w3a4">%</tc:token>
      <tc:token ID="w3a5">of</tc:token>
      <tc:token ID="w3a6">tokens</tc:token>
      <tc:token ID="w3a7">in</tc:token>
      <tc:token ID="w3a8">a</tc:token>
      <tc:token ID="w3a9">direct</tc:token>
      <tc:token ID="w3aa">speech</tc:token>
      <tc:token ID="w3ab">.</tc:token>
      <tc:token ID="w3ac">The</tc:token>
      <tc:token ID="w3ad">following</tc:token>
      <tc:token ID="w3ae">experiments</tc:token>
      <tc:token ID="w3af">use</tc:token>
      <tc:token ID="w3b0">both</tc:token>
      <tc:token ID="w3b1">labelled</tc:token>
      <tc:token ID="w3b2">subsets</tc:token>
      <tc:token ID="w3b3">described</tc:token>
      <tc:token ID="w3b4">above</tc:token>
      <tc:token ID="w3b5">,</tc:token>
      <tc:token ID="w3b6">the</tc:token>
      <tc:token ID="w3b7">large</tc:token>
      <tc:token ID="w3b8">Kerncorpus</tc:token>
      <tc:token ID="w3b9">and</tc:token>
      <tc:token ID="w3ba">the</tc:token>
      <tc:token ID="w3bb">smaller</tc:token>
      <tc:token ID="w3bc">ALB</tc:token>
      <tc:token ID="w3bd">.</tc:token>
      <tc:token ID="w3be">For</tc:token>
      <tc:token ID="w3bf">all</tc:token>
      <tc:token ID="w3c0">experiments</tc:token>
      <tc:token ID="w3c1">,</tc:token>
      <tc:token ID="w3c2">quotation</tc:token>
      <tc:token ID="w3c3">marks</tc:token>
      <tc:token ID="w3c4">are</tc:token>
      <tc:token ID="w3c5">removed</tc:token>
      <tc:token ID="w3c6">from</tc:token>
      <tc:token ID="w3c7">the</tc:token>
      <tc:token ID="w3c8">texts</tc:token>
      <tc:token ID="w3c9">.</tc:token>
      <tc:token ID="w3ca">This</tc:token>
      <tc:token ID="w3cb">is</tc:token>
      <tc:token ID="w3cc">done</tc:token>
      <tc:token ID="w3cd">to</tc:token>
      <tc:token ID="w3ce">avoid</tc:token>
      <tc:token ID="w3cf">training</tc:token>
      <tc:token ID="w3d0">models</tc:token>
      <tc:token ID="w3d1">that</tc:token>
      <tc:token ID="w3d2">rely</tc:token>
      <tc:token ID="w3d3">only</tc:token>
      <tc:token ID="w3d4">on</tc:token>
      <tc:token ID="w3d5">the</tc:token>
      <tc:token ID="w3d6">formal</tc:token>
      <tc:token ID="w3d7">style</tc:token>
      <tc:token ID="w3d8">of</tc:token>
      <tc:token ID="w3d9">qualifying</tc:token>
      <tc:token ID="w3da">direct</tc:token>
      <tc:token ID="w3db">speech</tc:token>
      <tc:token ID="w3dc">,</tc:token>
      <tc:token ID="w3dd">but</tc:token>
      <tc:token ID="w3de">also</tc:token>
      <tc:token ID="w3df">consider</tc:token>
      <tc:token ID="w3e0">implicit</tc:token>
      <tc:token ID="w3e1">signs</tc:token>
      <tc:token ID="w3e2">like</tc:token>
      <tc:token ID="w3e3">the</tc:token>
      <tc:token ID="w3e4">use</tc:token>
      <tc:token ID="w3e5">of</tc:token>
      <tc:token ID="w3e6">first</tc:token>
      <tc:token ID="w3e7">person</tc:token>
      <tc:token ID="w3e8">verbs</tc:token>
      <tc:token ID="w3e9">or</tc:token>
      <tc:token ID="w3ea">speech</tc:token>
      <tc:token ID="w3eb">words</tc:token>
      <tc:token ID="w3ec">.</tc:token>
      <tc:token ID="w3ed">We</tc:token>
      <tc:token ID="w3ee">conducted</tc:token>
      <tc:token ID="w3ef">experiments</tc:token>
      <tc:token ID="w3f0">on</tc:token>
      <tc:token ID="w3f1">two</tc:token>
      <tc:token ID="w3f2">different</tc:token>
      <tc:token ID="w3f3">levels</tc:token>
      <tc:token ID="w3f4">,</tc:token>
      <tc:token ID="w3f5">starting</tc:token>
      <tc:token ID="w3f6">with</tc:token>
      <tc:token ID="w3f7">a</tc:token>
      <tc:token ID="w3f8">sentence</tc:token>
      <tc:token ID="w3f9">classification</tc:token>
      <tc:token ID="w3fa">task</tc:token>
      <tc:token ID="w3fb">,</tc:token>
      <tc:token ID="w3fc">which</tc:token>
      <tc:token ID="w3fd">is</tc:token>
      <tc:token ID="w3fe">then</tc:token>
      <tc:token ID="w3ff">refined</tc:token>
      <tc:token ID="w400">to</tc:token>
      <tc:token ID="w401">detect</tc:token>
      <tc:token ID="w402">direct</tc:token>
      <tc:token ID="w403">speech</tc:token>
      <tc:token ID="w404">on</tc:token>
      <tc:token ID="w405">word-level</tc:token>
      <tc:token ID="w406">.</tc:token>
      <tc:token ID="w407">In</tc:token>
      <tc:token ID="w408">our</tc:token>
      <tc:token ID="w409">first</tc:token>
      <tc:token ID="w40a">classification</tc:token>
      <tc:token ID="w40b">task</tc:token>
      <tc:token ID="w40c">,</tc:token>
      <tc:token ID="w40d">documents</tc:token>
      <tc:token ID="w40e">are</tc:token>
      <tc:token ID="w40f">split</tc:token>
      <tc:token ID="w410">into</tc:token>
      <tc:token ID="w411">sentences</tc:token>
      <tc:token ID="w412">and</tc:token>
      <tc:token ID="w413">vectorised</tc:token>
      <tc:token ID="w414">by</tc:token>
      <tc:token ID="w415">storing</tc:token>
      <tc:token ID="w416">each</tc:token>
      <tc:token ID="w417">sentence</tc:token>
      <tc:token ID="w418">in</tc:token>
      <tc:token ID="w419">a</tc:token>
      <tc:token ID="w41a">bag-of-words</tc:token>
      <tc:token ID="w41b">representation</tc:token>
      <tc:token ID="w41c">.</tc:token>
      <tc:token ID="w41d">To</tc:token>
      <tc:token ID="w41e">create</tc:token>
      <tc:token ID="w41f">a</tc:token>
      <tc:token ID="w420">baseline</tc:token>
      <tc:token ID="w421">for</tc:token>
      <tc:token ID="w422">measuring</tc:token>
      <tc:token ID="w423">the</tc:token>
      <tc:token ID="w424">advantage</tc:token>
      <tc:token ID="w425">using</tc:token>
      <tc:token ID="w426">deep</tc:token>
      <tc:token ID="w427">learning</tc:token>
      <tc:token ID="w428">for</tc:token>
      <tc:token ID="w429">direct</tc:token>
      <tc:token ID="w42a">speech</tc:token>
      <tc:token ID="w42b">recognition</tc:token>
      <tc:token ID="w42c">,</tc:token>
      <tc:token ID="w42d">we</tc:token>
      <tc:token ID="w42e">compared</tc:token>
      <tc:token ID="w42f">the</tc:token>
      <tc:token ID="w430">performance</tc:token>
      <tc:token ID="w431">of</tc:token>
      <tc:token ID="w432">traditional</tc:token>
      <tc:token ID="w433">machine</tc:token>
      <tc:token ID="w434">learning</tc:token>
      <tc:token ID="w435">algorithms</tc:token>
      <tc:token ID="w436">on</tc:token>
      <tc:token ID="w437">our</tc:token>
      <tc:token ID="w438">labelled</tc:token>
      <tc:token ID="w439">datasets</tc:token>
      <tc:token ID="w43a">.</tc:token>
      <tc:token ID="w43b">Training</tc:token>
      <tc:token ID="w43c">and</tc:token>
      <tc:token ID="w43d">testing</tc:token>
      <tc:token ID="w43e">some</tc:token>
      <tc:token ID="w43f">of</tc:token>
      <tc:token ID="w440">the</tc:token>
      <tc:token ID="w441">most</tc:token>
      <tc:token ID="w442">common</tc:token>
      <tc:token ID="w443">machine</tc:token>
      <tc:token ID="w444">learning</tc:token>
      <tc:token ID="w445">classifiers</tc:token>
      <tc:token ID="w446">to</tc:token>
      <tc:token ID="w447">detect</tc:token>
      <tc:token ID="w448">sentences</tc:token>
      <tc:token ID="w449">containing</tc:token>
      <tc:token ID="w44a">at</tc:token>
      <tc:token ID="w44b">least</tc:token>
      <tc:token ID="w44c">one</tc:token>
      <tc:token ID="w44d">word</tc:token>
      <tc:token ID="w44e">of</tc:token>
      <tc:token ID="w44f">direct</tc:token>
      <tc:token ID="w450">speech</tc:token>
      <tc:token ID="w451">leads</tc:token>
      <tc:token ID="w452">to</tc:token>
      <tc:token ID="w453">an</tc:token>
      <tc:token ID="w454">accuracy</tc:token>
      <tc:token ID="w455">of</tc:token>
      <tc:token ID="w456">0.85</tc:token>
      <tc:token ID="w457">using</tc:token>
      <tc:token ID="w458">Logistic</tc:token>
      <tc:token ID="w459">Regression</tc:token>
      <tc:token ID="w45a">;</tc:token>
      <tc:token ID="w45b">for</tc:token>
      <tc:token ID="w45c">more</tc:token>
      <tc:token ID="w45d">results</tc:token>
      <tc:token ID="w45e">see</tc:token>
      <tc:token ID="w45f">Table</tc:token>
      <tc:token ID="w460">1.</tc:token>
      <tc:token ID="w461">Using</tc:token>
      <tc:token ID="w462">the</tc:token>
      <tc:token ID="w463">same</tc:token>
      <tc:token ID="w464">setting</tc:token>
      <tc:token ID="w465">and</tc:token>
      <tc:token ID="w466">replacing</tc:token>
      <tc:token ID="w467">machine</tc:token>
      <tc:token ID="w468">learning</tc:token>
      <tc:token ID="w469">with</tc:token>
      <tc:token ID="w46a">a</tc:token>
      <tc:token ID="w46b">combination</tc:token>
      <tc:token ID="w46c">of</tc:token>
      <tc:token ID="w46d">recurrent</tc:token>
      <tc:token ID="w46e">and</tc:token>
      <tc:token ID="w46f">convolutional</tc:token>
      <tc:token ID="w470">neural</tc:token>
      <tc:token ID="w471">networks</tc:token>
      <tc:token ID="w472">(</tc:token>
      <tc:token ID="w473">see</tc:token>
      <tc:token ID="w474">Chollet</tc:token>
      <tc:token ID="w475">2017</tc:token>
      <tc:token ID="w476">and</tc:token>
      <tc:token ID="w477">Goodfellow</tc:token>
      <tc:token ID="w478">2017</tc:token>
      <tc:token ID="w479">)</tc:token>
      <tc:token ID="w47a">ended</tc:token>
      <tc:token ID="w47b">up</tc:token>
      <tc:token ID="w47c">with</tc:token>
      <tc:token ID="w47d">an</tc:token>
      <tc:token ID="w47e">accuracy</tc:token>
      <tc:token ID="w47f">of</tc:token>
      <tc:token ID="w480">0.84</tc:token>
      <tc:token ID="w481">.</tc:token>
      <tc:token ID="w482">Since</tc:token>
      <tc:token ID="w483">we</tc:token>
      <tc:token ID="w484">noticed</tc:token>
      <tc:token ID="w485">that</tc:token>
      <tc:token ID="w486">three</tc:token>
      <tc:token ID="w487">of</tc:token>
      <tc:token ID="w488">our</tc:token>
      <tc:token ID="w489">classifiers</tc:token>
      <tc:token ID="w48a">all</tc:token>
      <tc:token ID="w48b">ended</tc:token>
      <tc:token ID="w48c">up</tc:token>
      <tc:token ID="w48d">with</tc:token>
      <tc:token ID="w48e">about</tc:token>
      <tc:token ID="w48f">the</tc:token>
      <tc:token ID="w490">same</tc:token>
      <tc:token ID="w491">score</tc:token>
      <tc:token ID="w492">,</tc:token>
      <tc:token ID="w493">we</tc:token>
      <tc:token ID="w494">decided</tc:token>
      <tc:token ID="w495">to</tc:token>
      <tc:token ID="w496">give</tc:token>
      <tc:token ID="w497">the</tc:token>
      <tc:token ID="w498">task</tc:token>
      <tc:token ID="w499">to</tc:token>
      <tc:token ID="w49a">two</tc:token>
      <tc:token ID="w49b">human</tc:token>
      <tc:token ID="w49c">annotators</tc:token>
      <tc:token ID="w49d">to</tc:token>
      <tc:token ID="w49e">establish</tc:token>
      <tc:token ID="w49f">an</tc:token>
      <tc:token ID="w4a0">upper</tc:token>
      <tc:token ID="w4a1">bound</tc:token>
      <tc:token ID="w4a2">.</tc:token>
      <tc:token ID="w4a3">We</tc:token>
      <tc:token ID="w4a4">selected</tc:token>
      <tc:token ID="w4a5">250</tc:token>
      <tc:token ID="w4a6">sentences</tc:token>
      <tc:token ID="w4a7">for</tc:token>
      <tc:token ID="w4a8">manual</tc:token>
      <tc:token ID="w4a9">annotation</tc:token>
      <tc:token ID="w4aa">and</tc:token>
      <tc:token ID="w4ab">again</tc:token>
      <tc:token ID="w4ac">removed</tc:token>
      <tc:token ID="w4ad">all</tc:token>
      <tc:token ID="w4ae">quotation</tc:token>
      <tc:token ID="w4af">marks</tc:token>
      <tc:token ID="w4b0">.</tc:token>
      <tc:token ID="w4b1">Both</tc:token>
      <tc:token ID="w4b2">annotators</tc:token>
      <tc:token ID="w4b3">ended</tc:token>
      <tc:token ID="w4b4">up</tc:token>
      <tc:token ID="w4b5">with</tc:token>
      <tc:token ID="w4b6">an</tc:token>
      <tc:token ID="w4b7">accuracy</tc:token>
      <tc:token ID="w4b8">comparable</tc:token>
      <tc:token ID="w4b9">to</tc:token>
      <tc:token ID="w4ba">that</tc:token>
      <tc:token ID="w4bb">of</tc:token>
      <tc:token ID="w4bc">the</tc:token>
      <tc:token ID="w4bd">best</tc:token>
      <tc:token ID="w4be">machine</tc:token>
      <tc:token ID="w4bf">learning</tc:token>
      <tc:token ID="w4c0">methods</tc:token>
      <tc:token ID="w4c1">,</tc:token>
      <tc:token ID="w4c2">84</tc:token>
      <tc:token ID="w4c3">%</tc:token>
      <tc:token ID="w4c4">and</tc:token>
      <tc:token ID="w4c5">82.8</tc:token>
      <tc:token ID="w4c6">%</tc:token>
      <tc:token ID="w4c7">respectively</tc:token>
      <tc:token ID="w4c8">.</tc:token>
      <tc:token ID="w4c9">From</tc:token>
      <tc:token ID="w4ca">this</tc:token>
      <tc:token ID="w4cb">result</tc:token>
      <tc:token ID="w4cc">we</tc:token>
      <tc:token ID="w4cd">concluded</tc:token>
      <tc:token ID="w4ce">that</tc:token>
      <tc:token ID="w4cf">it</tc:token>
      <tc:token ID="w4d0">is</tc:token>
      <tc:token ID="w4d1">not</tc:token>
      <tc:token ID="w4d2">expedient</tc:token>
      <tc:token ID="w4d3">to</tc:token>
      <tc:token ID="w4d4">further</tc:token>
      <tc:token ID="w4d5">optimise</tc:token>
      <tc:token ID="w4d6">the</tc:token>
      <tc:token ID="w4d7">sentence</tc:token>
      <tc:token ID="w4d8">classification</tc:token>
      <tc:token ID="w4d9">task</tc:token>
      <tc:token ID="w4da">,</tc:token>
      <tc:token ID="w4db">as</tc:token>
      <tc:token ID="w4dc">we</tc:token>
      <tc:token ID="w4dd">had</tc:token>
      <tc:token ID="w4de">already</tc:token>
      <tc:token ID="w4df">reached</tc:token>
      <tc:token ID="w4e0">human-level</tc:token>
      <tc:token ID="w4e1">accuracy</tc:token>
      <tc:token ID="w4e2">.</tc:token>
      <tc:token ID="w4e3">Because</tc:token>
      <tc:token ID="w4e4">of</tc:token>
      <tc:token ID="w4e5">the</tc:token>
      <tc:token ID="w4e6">results</tc:token>
      <tc:token ID="w4e7">from</tc:token>
      <tc:token ID="w4e8">the</tc:token>
      <tc:token ID="w4e9">previous</tc:token>
      <tc:token ID="w4ea">section</tc:token>
      <tc:token ID="w4eb">,</tc:token>
      <tc:token ID="w4ec">we</tc:token>
      <tc:token ID="w4ed">decided</tc:token>
      <tc:token ID="w4ee">to</tc:token>
      <tc:token ID="w4ef">modify</tc:token>
      <tc:token ID="w4f0">our</tc:token>
      <tc:token ID="w4f1">task</tc:token>
      <tc:token ID="w4f2">to</tc:token>
      <tc:token ID="w4f3">a</tc:token>
      <tc:token ID="w4f4">word-level</tc:token>
      <tc:token ID="w4f5">prediction</tc:token>
      <tc:token ID="w4f6">,</tc:token>
      <tc:token ID="w4f7">which</tc:token>
      <tc:token ID="w4f8">enables</tc:token>
      <tc:token ID="w4f9">us</tc:token>
      <tc:token ID="w4fa">to</tc:token>
      <tc:token ID="w4fb">include</tc:token>
      <tc:token ID="w4fc">more</tc:token>
      <tc:token ID="w4fd">context</tc:token>
      <tc:token ID="w4fe">by</tc:token>
      <tc:token ID="w4ff">ignoring</tc:token>
      <tc:token ID="w500">sentence</tc:token>
      <tc:token ID="w501">boundaries</tc:token>
      <tc:token ID="w502">and</tc:token>
      <tc:token ID="w503">at</tc:token>
      <tc:token ID="w504">the</tc:token>
      <tc:token ID="w505">same</tc:token>
      <tc:token ID="w506">time</tc:token>
      <tc:token ID="w507">make</tc:token>
      <tc:token ID="w508">more</tc:token>
      <tc:token ID="w509">fine-grained</tc:token>
      <tc:token ID="w50a">predictions</tc:token>
      <tc:token ID="w50b">.</tc:token>
      <tc:token ID="w50c">In</tc:token>
      <tc:token ID="w50d">this</tc:token>
      <tc:token ID="w50e">second</tc:token>
      <tc:token ID="w50f">classification</tc:token>
      <tc:token ID="w510">task</tc:token>
      <tc:token ID="w511">,</tc:token>
      <tc:token ID="w512">each</tc:token>
      <tc:token ID="w513">word</tc:token>
      <tc:token ID="w514">is</tc:token>
      <tc:token ID="w515">to</tc:token>
      <tc:token ID="w516">be</tc:token>
      <tc:token ID="w517">classified</tc:token>
      <tc:token ID="w518">separately</tc:token>
      <tc:token ID="w519">as</tc:token>
      <tc:token ID="w51a">inside</tc:token>
      <tc:token ID="w51b">or</tc:token>
      <tc:token ID="w51c">outside</tc:token>
      <tc:token ID="w51d">a</tc:token>
      <tc:token ID="w51e">direct</tc:token>
      <tc:token ID="w51f">speech</tc:token>
      <tc:token ID="w520">.</tc:token>
      <tc:token ID="w521">As</tc:token>
      <tc:token ID="w522">baseline</tc:token>
      <tc:token ID="w523">for</tc:token>
      <tc:token ID="w524">this</tc:token>
      <tc:token ID="w525">task</tc:token>
      <tc:token ID="w526">,</tc:token>
      <tc:token ID="w527">we</tc:token>
      <tc:token ID="w528">trained</tc:token>
      <tc:token ID="w529">a</tc:token>
      <tc:token ID="w52a">Linear</tc:token>
      <tc:token ID="w52b">Chain</tc:token>
      <tc:token ID="w52c">Conditional</tc:token>
      <tc:token ID="w52d">Random</tc:token>
      <tc:token ID="w52e">Field</tc:token>
      <tc:token ID="w52f">(</tc:token>
      <tc:token ID="w530">CRF</tc:token>
      <tc:token ID="w531">)</tc:token>
      <tc:token ID="w532">that</tc:token>
      <tc:token ID="w533">was</tc:token>
      <tc:token ID="w534">only</tc:token>
      <tc:token ID="w535">given</tc:token>
      <tc:token ID="w536">the</tc:token>
      <tc:token ID="w537">word</tc:token>
      <tc:token ID="w538">itself</tc:token>
      <tc:token ID="w539">and</tc:token>
      <tc:token ID="w53a">its</tc:token>
      <tc:token ID="w53b">part-of-speech</tc:token>
      <tc:token ID="w53c">tag</tc:token>
      <tc:token ID="w53d">.</tc:token>
      <tc:token ID="w53e">This</tc:token>
      <tc:token ID="w53f">CRF</tc:token>
      <tc:token ID="w540">stagnated</tc:token>
      <tc:token ID="w541">at</tc:token>
      <tc:token ID="w542">a</tc:token>
      <tc:token ID="w543">comparably</tc:token>
      <tc:token ID="w544">low</tc:token>
      <tc:token ID="w545">accuracy</tc:token>
      <tc:token ID="w546">of</tc:token>
      <tc:token ID="w547">0</tc:token>
      <tc:token ID="w548">.71</tc:token>
      <tc:token ID="w549">using</tc:token>
      <tc:token ID="w54a">cross-validation</tc:token>
      <tc:token ID="w54b">on</tc:token>
      <tc:token ID="w54c">the</tc:token>
      <tc:token ID="w54d">Kerncorpus</tc:token>
      <tc:token ID="w54e">.</tc:token>
      <tc:token ID="w54f">Since</tc:token>
      <tc:token ID="w550">our</tc:token>
      <tc:token ID="w551">goal</tc:token>
      <tc:token ID="w552">was</tc:token>
      <tc:token ID="w553">to</tc:token>
      <tc:token ID="w554">provide</tc:token>
      <tc:token ID="w555">the</tc:token>
      <tc:token ID="w556">classifier</tc:token>
      <tc:token ID="w557">with</tc:token>
      <tc:token ID="w558">more</tc:token>
      <tc:token ID="w559">context</tc:token>
      <tc:token ID="w55a">,</tc:token>
      <tc:token ID="w55b">we</tc:token>
      <tc:token ID="w55c">chose</tc:token>
      <tc:token ID="w55d">to</tc:token>
      <tc:token ID="w55e">use</tc:token>
      <tc:token ID="w55f">an</tc:token>
      <tc:token ID="w560">architecture</tc:token>
      <tc:token ID="w561">based</tc:token>
      <tc:token ID="w562">on</tc:token>
      <tc:token ID="w563">recurrent</tc:token>
      <tc:token ID="w564">neural</tc:token>
      <tc:token ID="w565">networks</tc:token>
      <tc:token ID="w566">,</tc:token>
      <tc:token ID="w567">which</tc:token>
      <tc:token ID="w568">are</tc:token>
      <tc:token ID="w569">able</tc:token>
      <tc:token ID="w56a">to</tc:token>
      <tc:token ID="w56b">deal</tc:token>
      <tc:token ID="w56c">with</tc:token>
      <tc:token ID="w56d">relatively</tc:token>
      <tc:token ID="w56e">large</tc:token>
      <tc:token ID="w56f">contexts</tc:token>
      <tc:token ID="w570">.</tc:token>
      <tc:token ID="w571">Our</tc:token>
      <tc:token ID="w572">assumption</tc:token>
      <tc:token ID="w573">here</tc:token>
      <tc:token ID="w574">is</tc:token>
      <tc:token ID="w575">that</tc:token>
      <tc:token ID="w576">,</tc:token>
      <tc:token ID="w577">for</tc:token>
      <tc:token ID="w578">a</tc:token>
      <tc:token ID="w579">good</tc:token>
      <tc:token ID="w57a">classification</tc:token>
      <tc:token ID="w57b">,</tc:token>
      <tc:token ID="w57c">we</tc:token>
      <tc:token ID="w57d">need</tc:token>
      <tc:token ID="w57e">context</tc:token>
      <tc:token ID="w57f">from</tc:token>
      <tc:token ID="w580">both</tc:token>
      <tc:token ID="w581">before</tc:token>
      <tc:token ID="w582">and</tc:token>
      <tc:token ID="w583">after</tc:token>
      <tc:token ID="w584">the</tc:token>
      <tc:token ID="w585">target</tc:token>
      <tc:token ID="w586">word</tc:token>
      <tc:token ID="w587">itself</tc:token>
      <tc:token ID="w588">,</tc:token>
      <tc:token ID="w589">as</tc:token>
      <tc:token ID="w58a">markers</tc:token>
      <tc:token ID="w58b">for</tc:token>
      <tc:token ID="w58c">direct</tc:token>
      <tc:token ID="w58d">speech</tc:token>
      <tc:token ID="w58e">can</tc:token>
      <tc:token ID="w58f">be</tc:token>
      <tc:token ID="w590">found</tc:token>
      <tc:token ID="w591">at</tc:token>
      <tc:token ID="w592">the</tc:token>
      <tc:token ID="w593">beginning</tc:token>
      <tc:token ID="w594">or</tc:token>
      <tc:token ID="w595">the</tc:token>
      <tc:token ID="w596">end</tc:token>
      <tc:token ID="w597">of</tc:token>
      <tc:token ID="w598">the</tc:token>
      <tc:token ID="w599">direct</tc:token>
      <tc:token ID="w59a">speech</tc:token>
      <tc:token ID="w59b">.</tc:token>
      <tc:token ID="w59c">We</tc:token>
      <tc:token ID="w59d">thus</tc:token>
      <tc:token ID="w59e">designed</tc:token>
      <tc:token ID="w59f">a</tc:token>
      <tc:token ID="w5a0">two-branch</tc:token>
      <tc:token ID="w5a1">network</tc:token>
      <tc:token ID="w5a2">,</tc:token>
      <tc:token ID="w5a3">visualised</tc:token>
      <tc:token ID="w5a4">in</tc:token>
      <tc:token ID="w5a5">Figure</tc:token>
      <tc:token ID="w5a6">1.</tc:token>
      <tc:token ID="w5a7">This</tc:token>
      <tc:token ID="w5a8">network</tc:token>
      <tc:token ID="w5a9">receives</tc:token>
      <tc:token ID="w5aa">as</tc:token>
      <tc:token ID="w5ab">input</tc:token>
      <tc:token ID="w5ac">a</tc:token>
      <tc:token ID="w5ad">text-segment</tc:token>
      <tc:token ID="w5ae">,</tc:token>
      <tc:token ID="w5af">specifically</tc:token>
      <tc:token ID="w5b0">the</tc:token>
      <tc:token ID="w5b1">target</tc:token>
      <tc:token ID="w5b2">word</tc:token>
      <tc:token ID="w5b3">in</tc:token>
      <tc:token ID="w5b4">its</tc:token>
      <tc:token ID="w5b5">context</tc:token>
      <tc:token ID="w5b6">.</tc:token>
      <tc:token ID="w5b7">The</tc:token>
      <tc:token ID="w5b8">words</tc:token>
      <tc:token ID="w5b9">of</tc:token>
      <tc:token ID="w5ba">the</tc:token>
      <tc:token ID="w5bb">input</tc:token>
      <tc:token ID="w5bc">are</tc:token>
      <tc:token ID="w5bd">then</tc:token>
      <tc:token ID="w5be">passed</tc:token>
      <tc:token ID="w5bf">through</tc:token>
      <tc:token ID="w5c0">an</tc:token>
      <tc:token ID="w5c1">embedding</tc:token>
      <tc:token ID="w5c2">layer</tc:token>
      <tc:token ID="w5c3">and</tc:token>
      <tc:token ID="w5c4">split</tc:token>
      <tc:token ID="w5c5">into</tc:token>
      <tc:token ID="w5c6">two</tc:token>
      <tc:token ID="w5c7">parts</tc:token>
      <tc:token ID="w5c8">,</tc:token>
      <tc:token ID="w5c9">where</tc:token>
      <tc:token ID="w5ca">the</tc:token>
      <tc:token ID="w5cb">first</tc:token>
      <tc:token ID="w5cc">part</tc:token>
      <tc:token ID="w5cd">contains</tc:token>
      <tc:token ID="w5ce">the</tc:token>
      <tc:token ID="w5cf">context</tc:token>
      <tc:token ID="w5d0">up</tc:token>
      <tc:token ID="w5d1">to</tc:token>
      <tc:token ID="w5d2">the</tc:token>
      <tc:token ID="w5d3">target</tc:token>
      <tc:token ID="w5d4">word</tc:token>
      <tc:token ID="w5d5">and</tc:token>
      <tc:token ID="w5d6">the</tc:token>
      <tc:token ID="w5d7">second</tc:token>
      <tc:token ID="w5d8">part</tc:token>
      <tc:token ID="w5d9">contains</tc:token>
      <tc:token ID="w5da">the</tc:token>
      <tc:token ID="w5db">context</tc:token>
      <tc:token ID="w5dc">following</tc:token>
      <tc:token ID="w5dd">the</tc:token>
      <tc:token ID="w5de">target</tc:token>
      <tc:token ID="w5df">word</tc:token>
      <tc:token ID="w5e0">.</tc:token>
      <tc:token ID="w5e1">The</tc:token>
      <tc:token ID="w5e2">target</tc:token>
      <tc:token ID="w5e3">word</tc:token>
      <tc:token ID="w5e4">itself</tc:token>
      <tc:token ID="w5e5">is</tc:token>
      <tc:token ID="w5e6">contained</tc:token>
      <tc:token ID="w5e7">in</tc:token>
      <tc:token ID="w5e8">both</tc:token>
      <tc:token ID="w5e9">parts</tc:token>
      <tc:token ID="w5ea">.</tc:token>
      <tc:token ID="w5eb">Each</tc:token>
      <tc:token ID="w5ec">part</tc:token>
      <tc:token ID="w5ed">is</tc:token>
      <tc:token ID="w5ee">passed</tc:token>
      <tc:token ID="w5ef">through</tc:token>
      <tc:token ID="w5f0">three</tc:token>
      <tc:token ID="w5f1">separate</tc:token>
      <tc:token ID="w5f2">LSTM-layers</tc:token>
      <tc:token ID="w5f3">.</tc:token>
      <tc:token ID="w5f4">In</tc:token>
      <tc:token ID="w5f5">the</tc:token>
      <tc:token ID="w5f6">future-branch</tc:token>
      <tc:token ID="w5f7">,</tc:token>
      <tc:token ID="w5f8">the</tc:token>
      <tc:token ID="w5f9">context</tc:token>
      <tc:token ID="w5fa">is</tc:token>
      <tc:token ID="w5fb">passed</tc:token>
      <tc:token ID="w5fc">through</tc:token>
      <tc:token ID="w5fd">the</tc:token>
      <tc:token ID="w5fe">layers</tc:token>
      <tc:token ID="w5ff">in</tc:token>
      <tc:token ID="w600">reverse</tc:token>
      <tc:token ID="w601">,</tc:token>
      <tc:token ID="w602">so</tc:token>
      <tc:token ID="w603">that</tc:token>
      <tc:token ID="w604">the</tc:token>
      <tc:token ID="w605">target</tc:token>
      <tc:token ID="w606">word</tc:token>
      <tc:token ID="w607">is</tc:token>
      <tc:token ID="w608">the</tc:token>
      <tc:token ID="w609">last</tc:token>
      <tc:token ID="w60a">word</tc:token>
      <tc:token ID="w60b">to</tc:token>
      <tc:token ID="w60c">be</tc:token>
      <tc:token ID="w60d">read</tc:token>
      <tc:token ID="w60e">in</tc:token>
      <tc:token ID="w60f">both</tc:token>
      <tc:token ID="w610">branches</tc:token>
      <tc:token ID="w611">.</tc:token>
      <tc:token ID="w612">The</tc:token>
      <tc:token ID="w613">LSTM-layers</tc:token>
      <tc:token ID="w614">in</tc:token>
      <tc:token ID="w615">the</tc:token>
      <tc:token ID="w616">past-branch</tc:token>
      <tc:token ID="w617">are</tc:token>
      <tc:token ID="w618">stateful</tc:token>
      <tc:token ID="w619">and</tc:token>
      <tc:token ID="w61a">can</tc:token>
      <tc:token ID="w61b">therefore</tc:token>
      <tc:token ID="w61c">theoretically</tc:token>
      <tc:token ID="w61d">retain</tc:token>
      <tc:token ID="w61e">the</tc:token>
      <tc:token ID="w61f">entire</tc:token>
      <tc:token ID="w620">context</tc:token>
      <tc:token ID="w621">of</tc:token>
      <tc:token ID="w622">the</tc:token>
      <tc:token ID="w623">novel</tc:token>
      <tc:token ID="w624">up</tc:token>
      <tc:token ID="w625">to</tc:token>
      <tc:token ID="w626">the</tc:token>
      <tc:token ID="w627">target</tc:token>
      <tc:token ID="w628">word</tc:token>
      <tc:token ID="w629">.</tc:token>
      <tc:token ID="w62a">The</tc:token>
      <tc:token ID="w62b">outputs</tc:token>
      <tc:token ID="w62c">of</tc:token>
      <tc:token ID="w62d">the</tc:token>
      <tc:token ID="w62e">final</tc:token>
      <tc:token ID="w62f">LSTM-layer</tc:token>
      <tc:token ID="w630">of</tc:token>
      <tc:token ID="w631">both</tc:token>
      <tc:token ID="w632">branches</tc:token>
      <tc:token ID="w633">are</tc:token>
      <tc:token ID="w634">concatenated</tc:token>
      <tc:token ID="w635">.</tc:token>
      <tc:token ID="w636">The</tc:token>
      <tc:token ID="w637">final</tc:token>
      <tc:token ID="w638">prediction</tc:token>
      <tc:token ID="w639">is</tc:token>
      <tc:token ID="w63a">made</tc:token>
      <tc:token ID="w63b">based</tc:token>
      <tc:token ID="w63c">on</tc:token>
      <tc:token ID="w63d">this</tc:token>
      <tc:token ID="w63e">concatenation</tc:token>
      <tc:token ID="w63f">by</tc:token>
      <tc:token ID="w640">a</tc:token>
      <tc:token ID="w641">fully</tc:token>
      <tc:token ID="w642">connected</tc:token>
      <tc:token ID="w643">layer</tc:token>
      <tc:token ID="w644">.</tc:token>
      <tc:token ID="w645">In</tc:token>
      <tc:token ID="w646">our</tc:token>
      <tc:token ID="w647">best</tc:token>
      <tc:token ID="w648">setup</tc:token>
      <tc:token ID="w649">,</tc:token>
      <tc:token ID="w64a">we</tc:token>
      <tc:token ID="w64b">used</tc:token>
      <tc:token ID="w64c">60</tc:token>
      <tc:token ID="w64d">words</tc:token>
      <tc:token ID="w64e">before</tc:token>
      <tc:token ID="w64f">and</tc:token>
      <tc:token ID="w650">after</tc:token>
      <tc:token ID="w651">the</tc:token>
      <tc:token ID="w652">target</tc:token>
      <tc:token ID="w653">word</tc:token>
      <tc:token ID="w654">as</tc:token>
      <tc:token ID="w655">context</tc:token>
      <tc:token ID="w656">.</tc:token>
      <tc:token ID="w657">Training</tc:token>
      <tc:token ID="w658">on</tc:token>
      <tc:token ID="w659">one</tc:token>
      <tc:token ID="w65a">half</tc:token>
      <tc:token ID="w65b">of</tc:token>
      <tc:token ID="w65c">the</tc:token>
      <tc:token ID="w65d">Kerncorpus</tc:token>
      <tc:token ID="w65e">and</tc:token>
      <tc:token ID="w65f">evaluating</tc:token>
      <tc:token ID="w660">on</tc:token>
      <tc:token ID="w661">the</tc:token>
      <tc:token ID="w662">other</tc:token>
      <tc:token ID="w663">half</tc:token>
      <tc:token ID="w664">,</tc:token>
      <tc:token ID="w665">this</tc:token>
      <tc:token ID="w666">setup</tc:token>
      <tc:token ID="w667">yielded</tc:token>
      <tc:token ID="w668">an</tc:token>
      <tc:token ID="w669">accuracy</tc:token>
      <tc:token ID="w66a">of</tc:token>
      <tc:token ID="w66b">0.83</tc:token>
      <tc:token ID="w66c">.</tc:token>
      <tc:token ID="w66d">Training</tc:token>
      <tc:token ID="w66e">on</tc:token>
      <tc:token ID="w66f">the</tc:token>
      <tc:token ID="w670">full</tc:token>
      <tc:token ID="w671">Kerncorpus</tc:token>
      <tc:token ID="w672">and</tc:token>
      <tc:token ID="w673">evaluating</tc:token>
      <tc:token ID="w674">on</tc:token>
      <tc:token ID="w675">the</tc:token>
      <tc:token ID="w676">manually</tc:token>
      <tc:token ID="w677">annotated</tc:token>
      <tc:token ID="w678">ALB</tc:token>
      <tc:token ID="w679">reached</tc:token>
      <tc:token ID="w67a">an</tc:token>
      <tc:token ID="w67b">even</tc:token>
      <tc:token ID="w67c">better</tc:token>
      <tc:token ID="w67d">accuracy</tc:token>
      <tc:token ID="w67e">of</tc:token>
      <tc:token ID="w67f">0.90</tc:token>
      <tc:token ID="w680">.</tc:token>
      <tc:token ID="w681">In</tc:token>
      <tc:token ID="w682">the</tc:token>
      <tc:token ID="w683">following</tc:token>
      <tc:token ID="w684">experiments</tc:token>
      <tc:token ID="w685">,</tc:token>
      <tc:token ID="w686">we</tc:token>
      <tc:token ID="w687">used</tc:token>
      <tc:token ID="w688">the</tc:token>
      <tc:token ID="w689">model</tc:token>
      <tc:token ID="w68a">based</tc:token>
      <tc:token ID="w68b">on</tc:token>
      <tc:token ID="w68c">the</tc:token>
      <tc:token ID="w68d">architecture</tc:token>
      <tc:token ID="w68e">described</tc:token>
      <tc:token ID="w68f">above</tc:token>
      <tc:token ID="w690">.</tc:token>
      <tc:token ID="w691">We</tc:token>
      <tc:token ID="w692">trained</tc:token>
      <tc:token ID="w693">this</tc:token>
      <tc:token ID="w694">model</tc:token>
      <tc:token ID="w695">on</tc:token>
      <tc:token ID="w696">the</tc:token>
      <tc:token ID="w697">Kerncorpus</tc:token>
      <tc:token ID="w698">and</tc:token>
      <tc:token ID="w699">used</tc:token>
      <tc:token ID="w69a">it</tc:token>
      <tc:token ID="w69b">to</tc:token>
      <tc:token ID="w69c">detect</tc:token>
      <tc:token ID="w69d">direct</tc:token>
      <tc:token ID="w69e">speech</tc:token>
      <tc:token ID="w69f">in</tc:token>
      <tc:token ID="w6a0">the</tc:token>
      <tc:token ID="w6a1">complete</tc:token>
      <tc:token ID="w6a2">corpora</tc:token>
      <tc:token ID="w6a3">PD</tc:token>
      <tc:token ID="w6a4">,</tc:token>
      <tc:token ID="w6a5">LB</tc:token>
      <tc:token ID="w6a6">and</tc:token>
      <tc:token ID="w6a7">HB</tc:token>
      <tc:token ID="w6a8">.</tc:token>
      <tc:token ID="w6a9">Here</tc:token>
      <tc:token ID="w6aa">,</tc:token>
      <tc:token ID="w6ab">we</tc:token>
      <tc:token ID="w6ac">describe</tc:token>
      <tc:token ID="w6ad">our</tc:token>
      <tc:token ID="w6ae">findings</tc:token>
      <tc:token ID="w6af">on</tc:token>
      <tc:token ID="w6b0">these</tc:token>
      <tc:token ID="w6b1">corpora</tc:token>
      <tc:token ID="w6b2">.</tc:token>
      <tc:token ID="w6b3">Figure</tc:token>
      <tc:token ID="w6b4">2</tc:token>
      <tc:token ID="w6b5">shows</tc:token>
      <tc:token ID="w6b6">the</tc:token>
      <tc:token ID="w6b7">ratio</tc:token>
      <tc:token ID="w6b8">of</tc:token>
      <tc:token ID="w6b9">direct</tc:token>
      <tc:token ID="w6ba">speech</tc:token>
      <tc:token ID="w6bb">in</tc:token>
      <tc:token ID="w6bc">German</tc:token>
      <tc:token ID="w6bd">novels</tc:token>
      <tc:token ID="w6be">from</tc:token>
      <tc:token ID="w6bf">1800</tc:token>
      <tc:token ID="w6c0">till</tc:token>
      <tc:token ID="w6c1">1900</tc:token>
      <tc:token ID="w6c2">based</tc:token>
      <tc:token ID="w6c3">on</tc:token>
      <tc:token ID="w6c4">the</tc:token>
      <tc:token ID="w6c5">texts</tc:token>
      <tc:token ID="w6c6">from</tc:token>
      <tc:token ID="w6c7">Corpus</tc:token>
      <tc:token ID="w6c8">PD</tc:token>
      <tc:token ID="w6c9">.</tc:token>
      <tc:token ID="w6ca">The</tc:token>
      <tc:token ID="w6cb">regression</tc:token>
      <tc:token ID="w6cc">line</tc:token>
      <tc:token ID="w6cd">indicates</tc:token>
      <tc:token ID="w6ce">a</tc:token>
      <tc:token ID="w6cf">decline</tc:token>
      <tc:token ID="w6d0">of</tc:token>
      <tc:token ID="w6d1">direct</tc:token>
      <tc:token ID="w6d2">speech</tc:token>
      <tc:token ID="w6d3">over</tc:token>
      <tc:token ID="w6d4">time</tc:token>
      <tc:token ID="w6d5">;</tc:token>
      <tc:token ID="w6d6">at</tc:token>
      <tc:token ID="w6d7">the</tc:token>
      <tc:token ID="w6d8">same</tc:token>
      <tc:token ID="w6d9">time</tc:token>
      <tc:token ID="w6da">,</tc:token>
      <tc:token ID="w6db">we</tc:token>
      <tc:token ID="w6dc">can</tc:token>
      <tc:token ID="w6dd">observe</tc:token>
      <tc:token ID="w6de">a</tc:token>
      <tc:token ID="w6df">decrease</tc:token>
      <tc:token ID="w6e0">of</tc:token>
      <tc:token ID="w6e1">variance</tc:token>
      <tc:token ID="w6e2">.</tc:token>
      <tc:token ID="w6e3">The</tc:token>
      <tc:token ID="w6e4">strong</tc:token>
      <tc:token ID="w6e5">variations</tc:token>
      <tc:token ID="w6e6">between</tc:token>
      <tc:token ID="w6e7">certain</tc:token>
      <tc:token ID="w6e8">years</tc:token>
      <tc:token ID="w6e9">,</tc:token>
      <tc:token ID="w6ea">especially</tc:token>
      <tc:token ID="w6eb">in</tc:token>
      <tc:token ID="w6ec">the</tc:token>
      <tc:token ID="w6ed">early</tc:token>
      <tc:token ID="w6ee">19th</tc:token>
      <tc:token ID="w6ef">century</tc:token>
      <tc:token ID="w6f0">,</tc:token>
      <tc:token ID="w6f1">are</tc:token>
      <tc:token ID="w6f2">caused</tc:token>
      <tc:token ID="w6f3">by</tc:token>
      <tc:token ID="w6f4">low</tc:token>
      <tc:token ID="w6f5">numbers</tc:token>
      <tc:token ID="w6f6">of</tc:token>
      <tc:token ID="w6f7">provided</tc:token>
      <tc:token ID="w6f8">texts</tc:token>
      <tc:token ID="w6f9">(</tc:token>
      <tc:token ID="w6fa">see</tc:token>
      <tc:token ID="w6fb">Fig.</tc:token>
      <tc:token ID="w6fc">3</tc:token>
      <tc:token ID="w6fd">)</tc:token>
      <tc:token ID="w6fe">.</tc:token>
      <tc:token ID="w6ff">For</tc:token>
      <tc:token ID="w700">instance</tc:token>
      <tc:token ID="w701">,</tc:token>
      <tc:token ID="w702">the</tc:token>
      <tc:token ID="w703">peak</tc:token>
      <tc:token ID="w704">in</tc:token>
      <tc:token ID="w705">1805</tc:token>
      <tc:token ID="w706">can</tc:token>
      <tc:token ID="w707">be</tc:token>
      <tc:token ID="w708">explained</tc:token>
      <tc:token ID="w709">by</tc:token>
      <tc:token ID="w70a">the</tc:token>
      <tc:token ID="w70b">first</tc:token>
      <tc:token ID="w70c">publication</tc:token>
      <tc:token ID="w70d">of</tc:token>
      <tc:token ID="w70e">Denis</tc:token>
      <tc:token ID="w70f">Diderots</tc:token>
      <tc:token ID="w710">”</tc:token>
      <tc:token ID="w711">Herrn</tc:token>
      <tc:token ID="w712">Rameaus</tc:token>
      <tc:token ID="w713">Neffe</tc:token>
      <tc:token ID="w714">”</tc:token>
      <tc:token ID="w715">,</tc:token>
      <tc:token ID="w716">a</tc:token>
      <tc:token ID="w717">philosophical</tc:token>
      <tc:token ID="w718">dialogue-based</tc:token>
      <tc:token ID="w719">novel</tc:token>
      <tc:token ID="w71a">.</tc:token>
      <tc:token ID="w71b">There</tc:token>
      <tc:token ID="w71c">is</tc:token>
      <tc:token ID="w71d">an</tc:token>
      <tc:token ID="w71e">assumption</tc:token>
      <tc:token ID="w71f">in</tc:token>
      <tc:token ID="w720">literary</tc:token>
      <tc:token ID="w721">studies</tc:token>
      <tc:token ID="w722">that</tc:token>
      <tc:token ID="w723">a</tc:token>
      <tc:token ID="w724">huge</tc:token>
      <tc:token ID="w725">amount</tc:token>
      <tc:token ID="w726">of</tc:token>
      <tc:token ID="w727">direct</tc:token>
      <tc:token ID="w728">speech</tc:token>
      <tc:token ID="w729">is</tc:token>
      <tc:token ID="w72a">an</tc:token>
      <tc:token ID="w72b">indicator</tc:token>
      <tc:token ID="w72c">of</tc:token>
      <tc:token ID="w72d">low</tc:token>
      <tc:token ID="w72e">brow</tc:token>
      <tc:token ID="w72f">fiction</tc:token>
      <tc:token ID="w730">.</tc:token>
      <tc:token ID="w731">Figure</tc:token>
      <tc:token ID="w732">4</tc:token>
      <tc:token ID="w733">shows</tc:token>
      <tc:token ID="w734">the</tc:token>
      <tc:token ID="w735">ratio</tc:token>
      <tc:token ID="w736">of</tc:token>
      <tc:token ID="w737">direct</tc:token>
      <tc:token ID="w738">speech</tc:token>
      <tc:token ID="w739">between</tc:token>
      <tc:token ID="w73a">Corpus</tc:token>
      <tc:token ID="w73b">LB</tc:token>
      <tc:token ID="w73c">and</tc:token>
      <tc:token ID="w73d">HB</tc:token>
      <tc:token ID="w73e">.</tc:token>
      <tc:token ID="w73f">While</tc:token>
      <tc:token ID="w740">the</tc:token>
      <tc:token ID="w741">mean</tc:token>
      <tc:token ID="w742">usage</tc:token>
      <tc:token ID="w743">of</tc:token>
      <tc:token ID="w744">direct</tc:token>
      <tc:token ID="w745">speech</tc:token>
      <tc:token ID="w746">is</tc:token>
      <tc:token ID="w747">nearly</tc:token>
      <tc:token ID="w748">equal</tc:token>
      <tc:token ID="w749">in</tc:token>
      <tc:token ID="w74a">both</tc:token>
      <tc:token ID="w74b">groups</tc:token>
      <tc:token ID="w74c">,</tc:token>
      <tc:token ID="w74d">the</tc:token>
      <tc:token ID="w74e">high</tc:token>
      <tc:token ID="w74f">brow</tc:token>
      <tc:token ID="w750">literature</tc:token>
      <tc:token ID="w751">is</tc:token>
      <tc:token ID="w752">far</tc:token>
      <tc:token ID="w753">more</tc:token>
      <tc:token ID="w754">variable</tc:token>
      <tc:token ID="w755">.</tc:token>
      <tc:token ID="w756">This</tc:token>
      <tc:token ID="w757">finding</tc:token>
      <tc:token ID="w758">is</tc:token>
      <tc:token ID="w759">contrary</tc:token>
      <tc:token ID="w75a">to</tc:token>
      <tc:token ID="w75b">the</tc:token>
      <tc:token ID="w75c">assumption</tc:token>
      <tc:token ID="w75d">mentioned</tc:token>
      <tc:token ID="w75e">above</tc:token>
      <tc:token ID="w75f">.</tc:token>
      <tc:token ID="w760">We</tc:token>
      <tc:token ID="w761">propose</tc:token>
      <tc:token ID="w762">that</tc:token>
      <tc:token ID="w763">,</tc:token>
      <tc:token ID="w764">while</tc:token>
      <tc:token ID="w765">there</tc:token>
      <tc:token ID="w766">is</tc:token>
      <tc:token ID="w767">no</tc:token>
      <tc:token ID="w768">clear</tc:token>
      <tc:token ID="w769">difference</tc:token>
      <tc:token ID="w76a">in</tc:token>
      <tc:token ID="w76b">the</tc:token>
      <tc:token ID="w76c">average</tc:token>
      <tc:token ID="w76d">use</tc:token>
      <tc:token ID="w76e">of</tc:token>
      <tc:token ID="w76f">direct</tc:token>
      <tc:token ID="w770">speech</tc:token>
      <tc:token ID="w771">between</tc:token>
      <tc:token ID="w772">high</tc:token>
      <tc:token ID="w773">and</tc:token>
      <tc:token ID="w774">low</tc:token>
      <tc:token ID="w775">brow</tc:token>
      <tc:token ID="w776">literature</tc:token>
      <tc:token ID="w777">,</tc:token>
      <tc:token ID="w778">authors</tc:token>
      <tc:token ID="w779">in</tc:token>
      <tc:token ID="w77a">high</tc:token>
      <tc:token ID="w77b">brow</tc:token>
      <tc:token ID="w77c">literature</tc:token>
      <tc:token ID="w77d">are</tc:token>
      <tc:token ID="w77e">far</tc:token>
      <tc:token ID="w77f">more</tc:token>
      <tc:token ID="w780">flexible</tc:token>
      <tc:token ID="w781">in</tc:token>
      <tc:token ID="w782">choosing</tc:token>
      <tc:token ID="w783">how</tc:token>
      <tc:token ID="w784">much</tc:token>
      <tc:token ID="w785">direct</tc:token>
      <tc:token ID="w786">speech</tc:token>
      <tc:token ID="w787">they</tc:token>
      <tc:token ID="w788">use</tc:token>
      <tc:token ID="w789">in</tc:token>
      <tc:token ID="w78a">their</tc:token>
      <tc:token ID="w78b">novels</tc:token>
      <tc:token ID="w78c">.</tc:token>
      <tc:token ID="w78d">Low</tc:token>
      <tc:token ID="w78e">brow</tc:token>
      <tc:token ID="w78f">literature</tc:token>
      <tc:token ID="w790">,</tc:token>
      <tc:token ID="w791">on</tc:token>
      <tc:token ID="w792">the</tc:token>
      <tc:token ID="w793">other</tc:token>
      <tc:token ID="w794">hand</tc:token>
      <tc:token ID="w795">,</tc:token>
      <tc:token ID="w796">is</tc:token>
      <tc:token ID="w797">expected</tc:token>
      <tc:token ID="w798">to</tc:token>
      <tc:token ID="w799">have</tc:token>
      <tc:token ID="w79a">a</tc:token>
      <tc:token ID="w79b">rather</tc:token>
      <tc:token ID="w79c">constant</tc:token>
      <tc:token ID="w79d">amount</tc:token>
      <tc:token ID="w79e">of</tc:token>
      <tc:token ID="w79f">dialogue</tc:token>
      <tc:token ID="w7a0">.</tc:token>
      <tc:token ID="w7a1">In</tc:token>
      <tc:token ID="w7a2">this</tc:token>
      <tc:token ID="w7a3">paper</tc:token>
      <tc:token ID="w7a4">,</tc:token>
      <tc:token ID="w7a5">we</tc:token>
      <tc:token ID="w7a6">introduced</tc:token>
      <tc:token ID="w7a7">a</tc:token>
      <tc:token ID="w7a8">neural</tc:token>
      <tc:token ID="w7a9">network</tc:token>
      <tc:token ID="w7aa">architecture</tc:token>
      <tc:token ID="w7ab">that</tc:token>
      <tc:token ID="w7ac">is</tc:token>
      <tc:token ID="w7ad">able</tc:token>
      <tc:token ID="w7ae">to</tc:token>
      <tc:token ID="w7af">learn</tc:token>
      <tc:token ID="w7b0">the</tc:token>
      <tc:token ID="w7b1">classification</tc:token>
      <tc:token ID="w7b2">of</tc:token>
      <tc:token ID="w7b3">direct</tc:token>
      <tc:token ID="w7b4">speech</tc:token>
      <tc:token ID="w7b5">by</tc:token>
      <tc:token ID="w7b6">training</tc:token>
      <tc:token ID="w7b7">on</tc:token>
      <tc:token ID="w7b8">weakly</tc:token>
      <tc:token ID="w7b9">labelled</tc:token>
      <tc:token ID="w7ba">data</tc:token>
      <tc:token ID="w7bb">.</tc:token>
      <tc:token ID="w7bc">This</tc:token>
      <tc:token ID="w7bd">network</tc:token>
      <tc:token ID="w7be">works</tc:token>
      <tc:token ID="w7bf">purely</tc:token>
      <tc:token ID="w7c0">on</tc:token>
      <tc:token ID="w7c1">the</tc:token>
      <tc:token ID="w7c2">raw</tc:token>
      <tc:token ID="w7c3">text</tc:token>
      <tc:token ID="w7c4">of</tc:token>
      <tc:token ID="w7c5">a</tc:token>
      <tc:token ID="w7c6">novel</tc:token>
      <tc:token ID="w7c7">by</tc:token>
      <tc:token ID="w7c8">taking</tc:token>
      <tc:token ID="w7c9">into</tc:token>
      <tc:token ID="w7ca">account</tc:token>
      <tc:token ID="w7cb">a</tc:token>
      <tc:token ID="w7cc">relatively</tc:token>
      <tc:token ID="w7cd">large</tc:token>
      <tc:token ID="w7ce">context</tc:token>
      <tc:token ID="w7cf">.</tc:token>
      <tc:token ID="w7d0">We</tc:token>
      <tc:token ID="w7d1">also</tc:token>
      <tc:token ID="w7d2">demonstrate</tc:token>
      <tc:token ID="w7d3">that</tc:token>
      <tc:token ID="w7d4">training</tc:token>
      <tc:token ID="w7d5">on</tc:token>
      <tc:token ID="w7d6">weakly</tc:token>
      <tc:token ID="w7d7">labelled</tc:token>
      <tc:token ID="w7d8">data</tc:token>
      <tc:token ID="w7d9">leads</tc:token>
      <tc:token ID="w7da">to</tc:token>
      <tc:token ID="w7db">satisfying</tc:token>
      <tc:token ID="w7dc">results</tc:token>
      <tc:token ID="w7dd">.</tc:token>
      <tc:token ID="w7de">While</tc:token>
      <tc:token ID="w7df">an</tc:token>
      <tc:token ID="w7e0">accuracy</tc:token>
      <tc:token ID="w7e1">of</tc:token>
      <tc:token ID="w7e2">0.9</tc:token>
      <tc:token ID="w7e3">is</tc:token>
      <tc:token ID="w7e4">remarkable</tc:token>
      <tc:token ID="w7e5">,</tc:token>
      <tc:token ID="w7e6">there</tc:token>
      <tc:token ID="w7e7">is</tc:token>
      <tc:token ID="w7e8">still</tc:token>
      <tc:token ID="w7e9">need</tc:token>
      <tc:token ID="w7ea">for</tc:token>
      <tc:token ID="w7eb">optimisation</tc:token>
      <tc:token ID="w7ec">.</tc:token>
      <tc:token ID="w7ed">Recent</tc:token>
      <tc:token ID="w7ee">developments</tc:token>
      <tc:token ID="w7ef">in</tc:token>
      <tc:token ID="w7f0">the</tc:token>
      <tc:token ID="w7f1">performance</tc:token>
      <tc:token ID="w7f2">of</tc:token>
      <tc:token ID="w7f3">neural</tc:token>
      <tc:token ID="w7f4">networks</tc:token>
      <tc:token ID="w7f5">by</tc:token>
      <tc:token ID="w7f6">adding</tc:token>
      <tc:token ID="w7f7">an</tc:token>
      <tc:token ID="w7f8">attention</tc:token>
      <tc:token ID="w7f9">mechanism</tc:token>
      <tc:token ID="w7fa">(</tc:token>
      <tc:token ID="w7fb">see</tc:token>
      <tc:token ID="w7fc">Rush</tc:token>
      <tc:token ID="w7fd">2015</tc:token>
      <tc:token ID="w7fe">)</tc:token>
      <tc:token ID="w7ff">could</tc:token>
      <tc:token ID="w800">improve</tc:token>
      <tc:token ID="w801">the</tc:token>
      <tc:token ID="w802">results</tc:token>
      <tc:token ID="w803">.</tc:token>
      <tc:token ID="w804">We</tc:token>
      <tc:token ID="w805">used</tc:token>
      <tc:token ID="w806">our</tc:token>
      <tc:token ID="w807">neural</tc:token>
      <tc:token ID="w808">network</tc:token>
      <tc:token ID="w809">to</tc:token>
      <tc:token ID="w80a">analyse</tc:token>
      <tc:token ID="w80b">the</tc:token>
      <tc:token ID="w80c">distribution</tc:token>
      <tc:token ID="w80d">of</tc:token>
      <tc:token ID="w80e">direct</tc:token>
      <tc:token ID="w80f">speech</tc:token>
      <tc:token ID="w810">over</tc:token>
      <tc:token ID="w811">time</tc:token>
      <tc:token ID="w812">and</tc:token>
      <tc:token ID="w813">genres</tc:token>
      <tc:token ID="w814">.</tc:token>
      <tc:token ID="w815">Besides</tc:token>
      <tc:token ID="w816">algorithmic</tc:token>
      <tc:token ID="w817">refinements</tc:token>
      <tc:token ID="w818">,</tc:token>
      <tc:token ID="w819">there</tc:token>
      <tc:token ID="w81a">is</tc:token>
      <tc:token ID="w81b">a</tc:token>
      <tc:token ID="w81c">lot</tc:token>
      <tc:token ID="w81d">of</tc:token>
      <tc:token ID="w81e">potential</tc:token>
      <tc:token ID="w81f">in</tc:token>
      <tc:token ID="w820">adding</tc:token>
      <tc:token ID="w821">more</tc:token>
      <tc:token ID="w822">text</tc:token>
      <tc:token ID="w823">to</tc:token>
      <tc:token ID="w824">our</tc:token>
      <tc:token ID="w825">corpus</tc:token>
      <tc:token ID="w826">and</tc:token>
      <tc:token ID="w827">refining</tc:token>
      <tc:token ID="w828">metadata</tc:token>
      <tc:token ID="w829">to</tc:token>
      <tc:token ID="w82a">allow</tc:token>
      <tc:token ID="w82b">more</tc:token>
      <tc:token ID="w82c">sophisticated</tc:token>
      <tc:token ID="w82d">research</tc:token>
      <tc:token ID="w82e">questions</tc:token>
      <tc:token ID="w82f">like</tc:token>
      <tc:token ID="w830">differences</tc:token>
      <tc:token ID="w831">between</tc:token>
      <tc:token ID="w832">or</tc:token>
      <tc:token ID="w833">development</tc:token>
      <tc:token ID="w834">of</tc:token>
      <tc:token ID="w835">direct</tc:token>
      <tc:token ID="w836">speech</tc:token>
      <tc:token ID="w837">in</tc:token>
      <tc:token ID="w838">certain</tc:token>
      <tc:token ID="w839">genres</tc:token>
      <tc:token ID="w83a">.</tc:token>
      <tc:token ID="w83b">Introduction</tc:token>
      <tc:token ID="w83c">Related</tc:token>
      <tc:token ID="w83d">Work</tc:token>
      <tc:token ID="w83e">and</tc:token>
      <tc:token ID="w83f">Task</tc:token>
      <tc:token ID="w840">Description</tc:token>
      <tc:token ID="w841">Corpus</tc:token>
      <tc:token ID="w842">and</tc:token>
      <tc:token ID="w843">Resources</tc:token>
      <tc:token ID="w844">Experiments</tc:token>
      <tc:token ID="w845">Sentence-Level</tc:token>
      <tc:token ID="w846">Classification</tc:token>
      <tc:token ID="w847">Word-Level</tc:token>
      <tc:token ID="w848">Classification</tc:token>
      <tc:token ID="w849">Figure</tc:token>
      <tc:token ID="w84a">1</tc:token>
      <tc:token ID="w84b">:</tc:token>
      <tc:token ID="w84c">Architecture</tc:token>
      <tc:token ID="w84d">of</tc:token>
      <tc:token ID="w84e">recurrent</tc:token>
      <tc:token ID="w84f">network</tc:token>
      <tc:token ID="w850">to</tc:token>
      <tc:token ID="w851">detect</tc:token>
      <tc:token ID="w852">direct</tc:token>
      <tc:token ID="w853">speech</tc:token>
      <tc:token ID="w854">.</tc:token>
      <tc:token ID="w855">Distribution</tc:token>
      <tc:token ID="w856">of</tc:token>
      <tc:token ID="w857">direct</tc:token>
      <tc:token ID="w858">speech</tc:token>
      <tc:token ID="w859">Direct</tc:token>
      <tc:token ID="w85a">speech</tc:token>
      <tc:token ID="w85b">in</tc:token>
      <tc:token ID="w85c">19th</tc:token>
      <tc:token ID="w85d">Century</tc:token>
      <tc:token ID="w85e">Fiction</tc:token>
      <tc:token ID="w85f">Figure</tc:token>
      <tc:token ID="w860">2</tc:token>
      <tc:token ID="w861">:</tc:token>
      <tc:token ID="w862">Ratio</tc:token>
      <tc:token ID="w863">of</tc:token>
      <tc:token ID="w864">direct</tc:token>
      <tc:token ID="w865">speech</tc:token>
      <tc:token ID="w866">in</tc:token>
      <tc:token ID="w867">German</tc:token>
      <tc:token ID="w868">novels</tc:token>
      <tc:token ID="w869">from</tc:token>
      <tc:token ID="w86a">1800</tc:token>
      <tc:token ID="w86b">–</tc:token>
      <tc:token ID="w86c">1900.</tc:token>
      <tc:token ID="w86d">Figure</tc:token>
      <tc:token ID="w86e">3</tc:token>
      <tc:token ID="w86f">:</tc:token>
      <tc:token ID="w870">Number</tc:token>
      <tc:token ID="w871">of</tc:token>
      <tc:token ID="w872">provided</tc:token>
      <tc:token ID="w873">novels</tc:token>
      <tc:token ID="w874">per</tc:token>
      <tc:token ID="w875">year</tc:token>
      <tc:token ID="w876">.</tc:token>
      <tc:token ID="w877">Distribution</tc:token>
      <tc:token ID="w878">of</tc:token>
      <tc:token ID="w879">direct</tc:token>
      <tc:token ID="w87a">speech</tc:token>
      <tc:token ID="w87b">in</tc:token>
      <tc:token ID="w87c">low</tc:token>
      <tc:token ID="w87d">and</tc:token>
      <tc:token ID="w87e">high</tc:token>
      <tc:token ID="w87f">brow</tc:token>
      <tc:token ID="w880">literature</tc:token>
      <tc:token ID="w881">Figure</tc:token>
      <tc:token ID="w882">4</tc:token>
      <tc:token ID="w883">:</tc:token>
      <tc:token ID="w884">Ratio</tc:token>
      <tc:token ID="w885">of</tc:token>
      <tc:token ID="w886">direct</tc:token>
      <tc:token ID="w887">speech</tc:token>
      <tc:token ID="w888">in</tc:token>
      <tc:token ID="w889">German</tc:token>
      <tc:token ID="w88a">low</tc:token>
      <tc:token ID="w88b">and</tc:token>
      <tc:token ID="w88c">high</tc:token>
      <tc:token ID="w88d">brow</tc:token>
      <tc:token ID="w88e">novels</tc:token>
      <tc:token ID="w88f">after</tc:token>
      <tc:token ID="w890">1945</tc:token>
      <tc:token ID="w891">.</tc:token>
      <tc:token ID="w892">Conclusion</tc:token>
      <tc:token ID="w893">and</tc:token>
      <tc:token ID="w894">Future</tc:token>
      <tc:token ID="w895">Work</tc:token>
      <tc:token ID="w896">Brunner</tc:token>
      <tc:token ID="w897">,</tc:token>
      <tc:token ID="w898">Annelen</tc:token>
      <tc:token ID="w899">(</tc:token>
      <tc:token ID="w89a">2013</tc:token>
      <tc:token ID="w89b">)</tc:token>
      <tc:token ID="w89c">:</tc:token>
      <tc:token ID="w89d">“</tc:token>
      <tc:token ID="w89e">Automatic</tc:token>
      <tc:token ID="w89f">recognition</tc:token>
      <tc:token ID="w8a0">of</tc:token>
      <tc:token ID="w8a1">speech</tc:token>
      <tc:token ID="w8a2">,</tc:token>
      <tc:token ID="w8a3">thought</tc:token>
      <tc:token ID="w8a4">,</tc:token>
      <tc:token ID="w8a5">and</tc:token>
      <tc:token ID="w8a6">writing</tc:token>
      <tc:token ID="w8a7">representation</tc:token>
      <tc:token ID="w8a8">in</tc:token>
      <tc:token ID="w8a9">German</tc:token>
      <tc:token ID="w8aa">narrative</tc:token>
      <tc:token ID="w8ab">texts</tc:token>
      <tc:token ID="w8ac">”</tc:token>
      <tc:token ID="w8ad">,</tc:token>
      <tc:token ID="w8ae">in</tc:token>
      <tc:token ID="w8af">Literary</tc:token>
      <tc:token ID="w8b0">and</tc:token>
      <tc:token ID="w8b1">Linguistic</tc:token>
      <tc:token ID="w8b2">Computing.</tc:token>
      <tc:token ID="w8b3">Vol.</tc:token>
      <tc:token ID="w8b4">28</tc:token>
      <tc:token ID="w8b5">(</tc:token>
      <tc:token ID="w8b6">2013</tc:token>
      <tc:token ID="w8b7">)</tc:token>
      <tc:token ID="w8b8">.</tc:token>
      <tc:token ID="w8b9">Chollet</tc:token>
      <tc:token ID="w8ba">,</tc:token>
      <tc:token ID="w8bb">Francois</tc:token>
      <tc:token ID="w8bc">(</tc:token>
      <tc:token ID="w8bd">2017</tc:token>
      <tc:token ID="w8be">)</tc:token>
      <tc:token ID="w8bf">:</tc:token>
      <tc:token ID="w8c0">“</tc:token>
      <tc:token ID="w8c1">Deep</tc:token>
      <tc:token ID="w8c2">Learning</tc:token>
      <tc:token ID="w8c3">with</tc:token>
      <tc:token ID="w8c4">Python</tc:token>
      <tc:token ID="w8c5">”</tc:token>
      <tc:token ID="w8c6">.</tc:token>
      <tc:token ID="w8c7">Manning</tc:token>
      <tc:token ID="w8c8">Publications</tc:token>
      <tc:token ID="w8c9">.</tc:token>
      <tc:token ID="w8ca">New</tc:token>
      <tc:token ID="w8cb">York</tc:token>
      <tc:token ID="w8cc">.</tc:token>
      <tc:token ID="w8cd">(</tc:token>
      <tc:token ID="w8ce">Preprint</tc:token>
      <tc:token ID="w8cf">:</tc:token>
      <tc:token ID="w8d0">https://www.manning.com/books/deep-learning-with-python)</tc:token>
      <tc:token ID="w8d1">Goodfellow</tc:token>
      <tc:token ID="w8d2">,</tc:token>
      <tc:token ID="w8d3">Ian</tc:token>
      <tc:token ID="w8d4">/</tc:token>
      <tc:token ID="w8d5">Bengio</tc:token>
      <tc:token ID="w8d6">Yoshua</tc:token>
      <tc:token ID="w8d7">/</tc:token>
      <tc:token ID="w8d8">Courville</tc:token>
      <tc:token ID="w8d9">,</tc:token>
      <tc:token ID="w8da">Aaron</tc:token>
      <tc:token ID="w8db">(</tc:token>
      <tc:token ID="w8dc">2016</tc:token>
      <tc:token ID="w8dd">)</tc:token>
      <tc:token ID="w8de">:</tc:token>
      <tc:token ID="w8df">“</tc:token>
      <tc:token ID="w8e0">Deep</tc:token>
      <tc:token ID="w8e1">Learning</tc:token>
      <tc:token ID="w8e2">”</tc:token>
      <tc:token ID="w8e3">.</tc:token>
      <tc:token ID="w8e4">MIT</tc:token>
      <tc:token ID="w8e5">Press</tc:token>
      <tc:token ID="w8e6">.</tc:token>
      <tc:token ID="w8e7">(</tc:token>
      <tc:token ID="w8e8">URL</tc:token>
      <tc:token ID="w8e9">:</tc:token>
      <tc:token ID="w8ea">http://www.deeplearningbook.org</tc:token>
      <tc:token ID="w8eb">)</tc:token>
      <tc:token ID="w8ec">Rush</tc:token>
      <tc:token ID="w8ed">,</tc:token>
      <tc:token ID="w8ee">Alexander</tc:token>
      <tc:token ID="w8ef">M.</tc:token>
      <tc:token ID="w8f0">/</tc:token>
      <tc:token ID="w8f1">Chopra</tc:token>
      <tc:token ID="w8f2">,</tc:token>
      <tc:token ID="w8f3">Sumit</tc:token>
      <tc:token ID="w8f4">/</tc:token>
      <tc:token ID="w8f5">Weston</tc:token>
      <tc:token ID="w8f6">,</tc:token>
      <tc:token ID="w8f7">Jason</tc:token>
      <tc:token ID="w8f8">(</tc:token>
      <tc:token ID="w8f9">2015</tc:token>
      <tc:token ID="w8fa">)</tc:token>
      <tc:token ID="w8fb">:</tc:token>
      <tc:token ID="w8fc">“</tc:token>
      <tc:token ID="w8fd">A</tc:token>
      <tc:token ID="w8fe">Neural</tc:token>
      <tc:token ID="w8ff">Attention</tc:token>
      <tc:token ID="w900">Model</tc:token>
      <tc:token ID="w901">for</tc:token>
      <tc:token ID="w902">Abstractive</tc:token>
      <tc:token ID="w903">Sentence</tc:token>
      <tc:token ID="w904">Summarization</tc:token>
      <tc:token ID="w905">”</tc:token>
      <tc:token ID="w906">.</tc:token>
      <tc:token ID="w907">arXiv</tc:token>
      <tc:token ID="w908">preprint</tc:token>
      <tc:token ID="w909">arXiv</tc:token>
      <tc:token ID="w90a">:</tc:token>
      <tc:token ID="w90b">1509</tc:token>
      <tc:token ID="w90c">.</tc:token>
      <tc:token ID="w90d">00685</tc:token>
      <tc:token ID="w90e">.</tc:token>
      <tc:token ID="w90f">Scheible</tc:token>
      <tc:token ID="w910">,</tc:token>
      <tc:token ID="w911">C.</tc:token>
      <tc:token ID="w912">,</tc:token>
      <tc:token ID="w913">Klinger</tc:token>
      <tc:token ID="w914">,</tc:token>
      <tc:token ID="w915">R.</tc:token>
      <tc:token ID="w916">&amp;</tc:token>
      <tc:token ID="w917">Padó</tc:token>
      <tc:token ID="w918">,</tc:token>
      <tc:token ID="w919">S.</tc:token>
      <tc:token ID="w91a">(</tc:token>
      <tc:token ID="w91b">2016</tc:token>
      <tc:token ID="w91c">)</tc:token>
      <tc:token ID="w91d">:</tc:token>
      <tc:token ID="w91e">“</tc:token>
      <tc:token ID="w91f">Model</tc:token>
      <tc:token ID="w920">Architectures</tc:token>
      <tc:token ID="w921">for</tc:token>
      <tc:token ID="w922">Quotation</tc:token>
      <tc:token ID="w923">Detection</tc:token>
      <tc:token ID="w924">”</tc:token>
      <tc:token ID="w925">,</tc:token>
      <tc:token ID="w926">in</tc:token>
      <tc:token ID="w927">Proceedings</tc:token>
      <tc:token ID="w928">of</tc:token>
      <tc:token ID="w929">ACL</tc:token>
      <tc:token ID="w92a">(</tc:token>
      <tc:token ID="w92b">p.</tc:token>
      <tc:token ID="w92c">/</tc:token>
      <tc:token ID="w92d">pp.</tc:token>
      <tc:token ID="w92e">1736–1745</tc:token>
      <tc:token ID="w92f">)</tc:token>
      <tc:token ID="w930">.</tc:token>
      <tc:token ID="w931">https://gutenberg.spiegel.de</tc:token>
      <tc:token ID="w932">We</tc:token>
      <tc:token ID="w933">cannot</tc:token>
      <tc:token ID="w934">use</tc:token>
      <tc:token ID="w935">the</tc:token>
      <tc:token ID="w936">remaining</tc:token>
      <tc:token ID="w937">texts</tc:token>
      <tc:token ID="w938">for</tc:token>
      <tc:token ID="w939">either</tc:token>
      <tc:token ID="w93a">training</tc:token>
      <tc:token ID="w93b">or</tc:token>
      <tc:token ID="w93c">evaluation</tc:token>
      <tc:token ID="w93d">,</tc:token>
      <tc:token ID="w93e">as</tc:token>
      <tc:token ID="w93f">we</tc:token>
      <tc:token ID="w940">do</tc:token>
      <tc:token ID="w941">not</tc:token>
      <tc:token ID="w942">have</tc:token>
      <tc:token ID="w943">any</tc:token>
      <tc:token ID="w944">reliable</tc:token>
      <tc:token ID="w945">source</tc:token>
      <tc:token ID="w946">of</tc:token>
      <tc:token ID="w947">labels</tc:token>
      <tc:token ID="w948">for</tc:token>
      <tc:token ID="w949">these</tc:token>
      <tc:token ID="w94a">texts</tc:token>
      <tc:token ID="w94b">.</tc:token>
      <tc:token ID="w94c">Bibliography</tc:token>
    </tc:tokens>
    <tc:sentences xmlns:tc="http://www.dspin.de/data/textcorpus">
      <tc:sentence tokenIDs="w1 w2 w3 w4 w5 w6 w7 w8 w9 wa wb wc wd we wf w10 w11" ID="s1"/>
      <tc:sentence tokenIDs="w12 w13 w14 w15 w16 w17 w18 w19 w1a w1b w1c w1d w1e w1f w20 w21 w22 w23 w24 w25 w26 w27 w28 w29 w2a" ID="s2"/>
      <tc:sentence tokenIDs="w2b w2c w2d w2e w2f w30 w31 w32 w33 w34 w35 w36 w37 w38 w39 w3a w3b w3c w3d w3e w3f" ID="s3"/>
      <tc:sentence tokenIDs="w40 w41 w42 w43 w44 w45 w46 w47 w48 w49 w4a w4b w4c w4d w4e w4f w50 w51 w52 w53 w54 w55 w56 w57 w58" ID="s4"/>
      <tc:sentence tokenIDs="w59 w5a w5b w5c w5d w5e w5f w60 w61 w62 w63 w64 w65 w66 w67 w68 w69 w6a w6b w6c w6d w6e w6f w70 w71 w72 w73 w74 w75 w76 w77 w78 w79 w7a w7b w7c" ID="s5"/>
      <tc:sentence tokenIDs="w7d w7e w7f w80 w81 w82 w83 w84 w85 w86 w87 w88 w89 w8a w8b w8c w8d w8e w8f w90 w91 w92 w93 w94 w95 w96 w97 w98 w99 w9a w9b w9c" ID="s6"/>
      <tc:sentence tokenIDs="w9d w9e w9f wa0 wa1 wa2 wa3 wa4 wa5 wa6 wa7 wa8 wa9 waa wab" ID="s7"/>
      <tc:sentence tokenIDs="wac wad wae waf wb0 wb1 wb2 wb3 wb4 wb5 wb6 wb7 wb8 wb9 wba wbb wbc wbd wbe wbf wc0 wc1 wc2 wc3 wc4 wc5 wc6 wc7 wc8 wc9" ID="s8"/>
      <tc:sentence tokenIDs="wca wcb wcc wcd wce wcf wd0 wd1 wd2 wd3 wd4 wd5 wd6 wd7 wd8 wd9 wda wdb wdc wdd wde wdf we0 we1 we2 we3 we4 we5" ID="s9"/>
      <tc:sentence tokenIDs="we6 we7 we8 we9 wea web wec wed wee wef wf0 wf1 wf2 wf3 wf4 wf5 wf6 wf7 wf8 wf9 wfa wfb wfc wfd wfe wff w100 w101" ID="sa"/>
      <tc:sentence tokenIDs="w102 w103 w104 w105 w106 w107 w108 w109 w10a w10b w10c w10d w10e w10f w110 w111 w112 w113 w114 w115 w116 w117 w118 w119 w11a w11b w11c" ID="sb"/>
      <tc:sentence tokenIDs="w11d w11e w11f w120 w121 w122 w123 w124 w125 w126 w127 w128 w129 w12a w12b w12c w12d w12e w12f w130 w131 w132" ID="sc"/>
      <tc:sentence tokenIDs="w133 w134 w135 w136 w137 w138 w139 w13a w13b w13c w13d w13e w13f w140 w141 w142 w143 w144 w145 w146 w147 w148 w149 w14a w14b w14c w14d w14e w14f w150 w151 w152 w153 w154 w155 w156 w157 w158 w159 w15a w15b w15c w15d w15e" ID="sd"/>
      <tc:sentence tokenIDs="w15f w160 w161 w162 w163 w164 w165 w166 w167 w168 w169 w16a w16b w16c w16d" ID="se"/>
      <tc:sentence tokenIDs="w16e w16f w170 w171 w172 w173 w174 w175 w176 w177 w178 w179 w17a w17b w17c w17d w17e w17f w180 w181 w182 w183 w184 w185 w186 w187" ID="sf"/>
      <tc:sentence tokenIDs="w188 w189 w18a w18b w18c w18d w18e w18f w190 w191 w192 w193 w194 w195 w196 w197 w198 w199 w19a w19b" ID="s10"/>
      <tc:sentence tokenIDs="w19c w19d w19e w19f w1a0 w1a1 w1a2 w1a3 w1a4 w1a5 w1a6 w1a7 w1a8 w1a9 w1aa w1ab w1ac w1ad w1ae w1af w1b0 w1b1 w1b2 w1b3 w1b4 w1b5 w1b6 w1b7 w1b8 w1b9 w1ba" ID="s11"/>
      <tc:sentence tokenIDs="w1bb w1bc w1bd w1be w1bf w1c0 w1c1 w1c2 w1c3 w1c4 w1c5 w1c6 w1c7 w1c8 w1c9 w1ca w1cb w1cc w1cd w1ce" ID="s12"/>
      <tc:sentence tokenIDs="w1cf w1d0 w1d1 w1d2 w1d3 w1d4 w1d5 w1d6 w1d7 w1d8 w1d9 w1da w1db w1dc w1dd w1de w1df w1e0 w1e1 w1e2 w1e3 w1e4" ID="s13"/>
      <tc:sentence tokenIDs="w1e5 w1e6 w1e7 w1e8 w1e9 w1ea w1eb w1ec w1ed w1ee w1ef w1f0 w1f1 w1f2 w1f3 w1f4 w1f5 w1f6 w1f7 w1f8 w1f9 w1fa w1fb w1fc w1fd w1fe w1ff w200 w201 w202 w203 w204 w205 w206 w207 w208 w209 w20a w20b w20c w20d w20e w20f w210 w211 w212 w213 w214 w215 w216 w217 w218 w219 w21a w21b w21c w21d w21e w21f w220 w221 w222 w223 w224 w225 w226 w227 w228 w229 w22a" ID="s14"/>
      <tc:sentence tokenIDs="w22b w22c w22d w22e w22f w230 w231 w232 w233 w234 w235 w236 w237 w238 w239 w23a w23b w23c w23d w23e w23f" ID="s15"/>
      <tc:sentence tokenIDs="w240 w241 w242 w243 w244 w245 w246 w247 w248 w249" ID="s16"/>
      <tc:sentence tokenIDs="w24a w24b w24c w24d w24e w24f w250 w251 w252 w253 w254 w255 w256 w257 w258 w259 w25a w25b w25c w25d w25e w25f w260 w261 w262 w263 w264 w265 w266 w267 w268 w269 w26a w26b w26c w26d" ID="s17"/>
      <tc:sentence tokenIDs="w26e w26f w270 w271 w272 w273 w274 w275 w276 w277 w278 w279 w27a w27b w27c w27d w27e w27f w280 w281 w282 w283 w284 w285" ID="s18"/>
      <tc:sentence tokenIDs="w286 w287 w288 w289 w28a w28b w28c w28d w28e w28f w290 w291 w292 w293 w294 w295 w296 w297 w298 w299 w29a w29b w29c w29d w29e w29f w2a0 w2a1" ID="s19"/>
      <tc:sentence tokenIDs="w2a2 w2a3 w2a4 w2a5 w2a6 w2a7 w2a8 w2a9 w2aa w2ab w2ac w2ad w2ae w2af w2b0 w2b1 w2b2 w2b3 w2b4 w2b5 w2b6 w2b7 w2b8 w2b9" ID="s1a"/>
      <tc:sentence tokenIDs="w2ba w2bb w2bc w2bd w2be w2bf w2c0 w2c1 w2c2 w2c3 w2c4 w2c5 w2c6 w2c7 w2c8 w2c9 w2ca w2cb w2cc w2cd w2ce w2cf w2d0 w2d1 w2d2 w2d3 w2d4 w2d5 w2d6 w2d7 w2d8 w2d9 w2da w2db w2dc w2dd w2de w2df" ID="s1b"/>
      <tc:sentence tokenIDs="w2e0 w2e1 w2e2 w2e3 w2e4 w2e5 w2e6 w2e7 w2e8 w2e9 w2ea w2eb w2ec w2ed w2ee w2ef w2f0 w2f1 w2f2" ID="s1c"/>
      <tc:sentence tokenIDs="w2f3 w2f4 w2f5 w2f6 w2f7 w2f8 w2f9 w2fa w2fb w2fc w2fd w2fe w2ff w300 w301 w302" ID="s1d"/>
      <tc:sentence tokenIDs="w303 w304 w305 w306 w307 w308 w309 w30a w30b w30c w30d w30e w30f w310 w311 w312 w313 w314 w315 w316 w317 w318 w319 w31a w31b w31c w31d w31e w31f w320 w321 w322 w323 w324 w325" ID="s1e"/>
      <tc:sentence tokenIDs="w326 w327 w328 w329 w32a w32b w32c w32d w32e w32f w330 w331 w332 w333 w334 w335 w336 w337 w338 w339 w33a w33b w33c" ID="s1f"/>
      <tc:sentence tokenIDs="w33d w33e w33f w340 w341 w342 w343 w344 w345 w346 w347 w348 w349 w34a w34b w34c w34d w34e w34f w350 w351 w352 w353 w354" ID="s20"/>
      <tc:sentence tokenIDs="w355 w356 w357 w358 w359 w35a w35b w35c w35d w35e w35f w360 w361 w362 w363 w364 w365 w366 w367 w368 w369 w36a w36b w36c w36d w36e w36f w370 w371" ID="s21"/>
      <tc:sentence tokenIDs="w372 w373 w374 w375 w376 w377 w378 w379 w37a w37b w37c w37d w37e w37f w380 w381 w382 w383" ID="s22"/>
      <tc:sentence tokenIDs="w384 w385 w386 w387 w388 w389 w38a w38b w38c w38d w38e" ID="s23"/>
      <tc:sentence tokenIDs="w38f w390 w391 w392 w393 w394 w395 w396 w397 w398 w399 w39a w39b w39c w39d w39e w39f w3a0 w3a1 w3a2 w3a3 w3a4 w3a5 w3a6 w3a7 w3a8 w3a9 w3aa w3ab" ID="s24"/>
      <tc:sentence tokenIDs="w3ac w3ad w3ae w3af w3b0 w3b1 w3b2 w3b3 w3b4 w3b5 w3b6 w3b7 w3b8 w3b9 w3ba w3bb w3bc w3bd" ID="s25"/>
      <tc:sentence tokenIDs="w3be w3bf w3c0 w3c1 w3c2 w3c3 w3c4 w3c5 w3c6 w3c7 w3c8 w3c9" ID="s26"/>
      <tc:sentence tokenIDs="w3ca w3cb w3cc w3cd w3ce w3cf w3d0 w3d1 w3d2 w3d3 w3d4 w3d5 w3d6 w3d7 w3d8 w3d9 w3da w3db w3dc w3dd w3de w3df w3e0 w3e1 w3e2 w3e3 w3e4 w3e5 w3e6 w3e7 w3e8 w3e9 w3ea w3eb w3ec" ID="s27"/>
      <tc:sentence tokenIDs="w3ed w3ee w3ef w3f0 w3f1 w3f2 w3f3 w3f4 w3f5 w3f6 w3f7 w3f8 w3f9 w3fa w3fb w3fc w3fd w3fe w3ff w400 w401 w402 w403 w404 w405 w406" ID="s28"/>
      <tc:sentence tokenIDs="w407 w408 w409 w40a w40b w40c w40d w40e w40f w410 w411 w412 w413 w414 w415 w416 w417 w418 w419 w41a w41b w41c" ID="s29"/>
      <tc:sentence tokenIDs="w41d w41e w41f w420 w421 w422 w423 w424 w425 w426 w427 w428 w429 w42a w42b w42c w42d w42e w42f w430 w431 w432 w433 w434 w435 w436 w437 w438 w439 w43a" ID="s2a"/>
      <tc:sentence tokenIDs="w43b w43c w43d w43e w43f w440 w441 w442 w443 w444 w445 w446 w447 w448 w449 w44a w44b w44c w44d w44e w44f w450 w451 w452 w453 w454 w455 w456 w457 w458 w459 w45a w45b w45c w45d w45e w45f w460 w461 w462 w463 w464 w465 w466 w467 w468 w469 w46a w46b w46c w46d w46e w46f w470 w471 w472 w473 w474 w475 w476 w477 w478 w479 w47a w47b w47c w47d w47e w47f w480 w481" ID="s2b"/>
      <tc:sentence tokenIDs="w482 w483 w484 w485 w486 w487 w488 w489 w48a w48b w48c w48d w48e w48f w490 w491 w492 w493 w494 w495 w496 w497 w498 w499 w49a w49b w49c w49d w49e w49f w4a0 w4a1 w4a2" ID="s2c"/>
      <tc:sentence tokenIDs="w4a3 w4a4 w4a5 w4a6 w4a7 w4a8 w4a9 w4aa w4ab w4ac w4ad w4ae w4af w4b0" ID="s2d"/>
      <tc:sentence tokenIDs="w4b1 w4b2 w4b3 w4b4 w4b5 w4b6 w4b7 w4b8 w4b9 w4ba w4bb w4bc w4bd w4be w4bf w4c0 w4c1 w4c2 w4c3 w4c4 w4c5 w4c6 w4c7 w4c8" ID="s2e"/>
      <tc:sentence tokenIDs="w4c9 w4ca w4cb w4cc w4cd w4ce w4cf w4d0 w4d1 w4d2 w4d3 w4d4 w4d5 w4d6 w4d7 w4d8 w4d9 w4da w4db w4dc w4dd w4de w4df w4e0 w4e1 w4e2" ID="s2f"/>
      <tc:sentence tokenIDs="w4e3 w4e4 w4e5 w4e6 w4e7 w4e8 w4e9 w4ea w4eb w4ec w4ed w4ee w4ef w4f0 w4f1 w4f2 w4f3 w4f4 w4f5 w4f6 w4f7 w4f8 w4f9 w4fa w4fb w4fc w4fd w4fe w4ff w500 w501 w502 w503 w504 w505 w506 w507 w508 w509 w50a w50b" ID="s30"/>
      <tc:sentence tokenIDs="w50c w50d w50e w50f w510 w511 w512 w513 w514 w515 w516 w517 w518 w519 w51a w51b w51c w51d w51e w51f w520" ID="s31"/>
      <tc:sentence tokenIDs="w521 w522 w523 w524 w525 w526 w527 w528 w529 w52a w52b w52c w52d w52e w52f w530 w531 w532 w533 w534 w535 w536 w537 w538 w539 w53a w53b w53c w53d" ID="s32"/>
      <tc:sentence tokenIDs="w53e w53f w540 w541 w542 w543 w544 w545 w546 w547 w548 w549 w54a w54b w54c w54d w54e" ID="s33"/>
      <tc:sentence tokenIDs="w54f w550 w551 w552 w553 w554 w555 w556 w557 w558 w559 w55a w55b w55c w55d w55e w55f w560 w561 w562 w563 w564 w565 w566 w567 w568 w569 w56a w56b w56c w56d w56e w56f w570" ID="s34"/>
      <tc:sentence tokenIDs="w571 w572 w573 w574 w575 w576 w577 w578 w579 w57a w57b w57c w57d w57e w57f w580 w581 w582 w583 w584 w585 w586 w587 w588 w589 w58a w58b w58c w58d w58e w58f w590 w591 w592 w593 w594 w595 w596 w597 w598 w599 w59a w59b" ID="s35"/>
      <tc:sentence tokenIDs="w59c w59d w59e w59f w5a0 w5a1 w5a2 w5a3 w5a4 w5a5 w5a6 w5a7 w5a8 w5a9 w5aa w5ab w5ac w5ad w5ae w5af w5b0 w5b1 w5b2 w5b3 w5b4 w5b5 w5b6" ID="s36"/>
      <tc:sentence tokenIDs="w5b7 w5b8 w5b9 w5ba w5bb w5bc w5bd w5be w5bf w5c0 w5c1 w5c2 w5c3 w5c4 w5c5 w5c6 w5c7 w5c8 w5c9 w5ca w5cb w5cc w5cd w5ce w5cf w5d0 w5d1 w5d2 w5d3 w5d4 w5d5 w5d6 w5d7 w5d8 w5d9 w5da w5db w5dc w5dd w5de w5df w5e0" ID="s37"/>
      <tc:sentence tokenIDs="w5e1 w5e2 w5e3 w5e4 w5e5 w5e6 w5e7 w5e8 w5e9 w5ea" ID="s38"/>
      <tc:sentence tokenIDs="w5eb w5ec w5ed w5ee w5ef w5f0 w5f1 w5f2 w5f3" ID="s39"/>
      <tc:sentence tokenIDs="w5f4 w5f5 w5f6 w5f7 w5f8 w5f9 w5fa w5fb w5fc w5fd w5fe w5ff w600 w601 w602 w603 w604 w605 w606 w607 w608 w609 w60a w60b w60c w60d w60e w60f w610 w611" ID="s3a"/>
      <tc:sentence tokenIDs="w612 w613 w614 w615 w616 w617 w618 w619 w61a w61b w61c w61d w61e w61f w620 w621 w622 w623 w624 w625 w626 w627 w628 w629" ID="s3b"/>
      <tc:sentence tokenIDs="w62a w62b w62c w62d w62e w62f w630 w631 w632 w633 w634 w635" ID="s3c"/>
      <tc:sentence tokenIDs="w636 w637 w638 w639 w63a w63b w63c w63d w63e w63f w640 w641 w642 w643 w644" ID="s3d"/>
      <tc:sentence tokenIDs="w645 w646 w647 w648 w649 w64a w64b w64c w64d w64e w64f w650 w651 w652 w653 w654 w655 w656" ID="s3e"/>
      <tc:sentence tokenIDs="w657 w658 w659 w65a w65b w65c w65d w65e w65f w660 w661 w662 w663 w664 w665 w666 w667 w668 w669 w66a w66b w66c" ID="s3f"/>
      <tc:sentence tokenIDs="w66d w66e w66f w670 w671 w672 w673 w674 w675 w676 w677 w678 w679 w67a w67b w67c w67d w67e w67f w680" ID="s40"/>
      <tc:sentence tokenIDs="w681 w682 w683 w684 w685 w686 w687 w688 w689 w68a w68b w68c w68d w68e w68f w690" ID="s41"/>
      <tc:sentence tokenIDs="w691 w692 w693 w694 w695 w696 w697 w698 w699 w69a w69b w69c w69d w69e w69f w6a0 w6a1 w6a2 w6a3 w6a4 w6a5 w6a6 w6a7 w6a8" ID="s42"/>
      <tc:sentence tokenIDs="w6a9 w6aa w6ab w6ac w6ad w6ae w6af w6b0 w6b1 w6b2" ID="s43"/>
      <tc:sentence tokenIDs="w6b3 w6b4 w6b5 w6b6 w6b7 w6b8 w6b9 w6ba w6bb w6bc w6bd w6be w6bf w6c0 w6c1 w6c2 w6c3 w6c4 w6c5 w6c6 w6c7 w6c8 w6c9" ID="s44"/>
      <tc:sentence tokenIDs="w6ca w6cb w6cc w6cd w6ce w6cf w6d0 w6d1 w6d2 w6d3 w6d4 w6d5 w6d6 w6d7 w6d8 w6d9 w6da w6db w6dc w6dd w6de w6df w6e0 w6e1 w6e2" ID="s45"/>
      <tc:sentence tokenIDs="w6e3 w6e4 w6e5 w6e6 w6e7 w6e8 w6e9 w6ea w6eb w6ec w6ed w6ee w6ef w6f0 w6f1 w6f2 w6f3 w6f4 w6f5 w6f6 w6f7 w6f8 w6f9 w6fa w6fb w6fc w6fd w6fe" ID="s46"/>
      <tc:sentence tokenIDs="w6ff w700 w701 w702 w703 w704 w705 w706 w707 w708 w709 w70a w70b w70c w70d w70e w70f w710 w711 w712 w713 w714 w715 w716 w717 w718 w719 w71a" ID="s47"/>
      <tc:sentence tokenIDs="w71b w71c w71d w71e w71f w720 w721 w722 w723 w724 w725 w726 w727 w728 w729 w72a w72b w72c w72d w72e w72f w730" ID="s48"/>
      <tc:sentence tokenIDs="w731 w732 w733 w734 w735 w736 w737 w738 w739 w73a w73b w73c w73d w73e" ID="s49"/>
      <tc:sentence tokenIDs="w73f w740 w741 w742 w743 w744 w745 w746 w747 w748 w749 w74a w74b w74c w74d w74e w74f w750 w751 w752 w753 w754 w755" ID="s4a"/>
      <tc:sentence tokenIDs="w756 w757 w758 w759 w75a w75b w75c w75d w75e w75f" ID="s4b"/>
      <tc:sentence tokenIDs="w760 w761 w762 w763 w764 w765 w766 w767 w768 w769 w76a w76b w76c w76d w76e w76f w770 w771 w772 w773 w774 w775 w776 w777 w778 w779 w77a w77b w77c w77d w77e w77f w780 w781 w782 w783 w784 w785 w786 w787 w788 w789 w78a w78b w78c" ID="s4c"/>
      <tc:sentence tokenIDs="w78d w78e w78f w790 w791 w792 w793 w794 w795 w796 w797 w798 w799 w79a w79b w79c w79d w79e w79f w7a0" ID="s4d"/>
      <tc:sentence tokenIDs="w7a1 w7a2 w7a3 w7a4 w7a5 w7a6 w7a7 w7a8 w7a9 w7aa w7ab w7ac w7ad w7ae w7af w7b0 w7b1 w7b2 w7b3 w7b4 w7b5 w7b6 w7b7 w7b8 w7b9 w7ba w7bb" ID="s4e"/>
      <tc:sentence tokenIDs="w7bc w7bd w7be w7bf w7c0 w7c1 w7c2 w7c3 w7c4 w7c5 w7c6 w7c7 w7c8 w7c9 w7ca w7cb w7cc w7cd w7ce w7cf" ID="s4f"/>
      <tc:sentence tokenIDs="w7d0 w7d1 w7d2 w7d3 w7d4 w7d5 w7d6 w7d7 w7d8 w7d9 w7da w7db w7dc w7dd" ID="s50"/>
      <tc:sentence tokenIDs="w7de w7df w7e0 w7e1 w7e2 w7e3 w7e4 w7e5 w7e6 w7e7 w7e8 w7e9 w7ea w7eb w7ec" ID="s51"/>
      <tc:sentence tokenIDs="w7ed w7ee w7ef w7f0 w7f1 w7f2 w7f3 w7f4 w7f5 w7f6 w7f7 w7f8 w7f9 w7fa w7fb w7fc w7fd w7fe w7ff w800 w801 w802 w803" ID="s52"/>
      <tc:sentence tokenIDs="w804 w805 w806 w807 w808 w809 w80a w80b w80c w80d w80e w80f w810 w811 w812 w813 w814" ID="s53"/>
      <tc:sentence tokenIDs="w815 w816 w817 w818 w819 w81a w81b w81c w81d w81e w81f w820 w821 w822 w823 w824 w825 w826 w827 w828 w829 w82a w82b w82c w82d w82e w82f w830 w831 w832 w833 w834 w835 w836 w837 w838 w839 w83a" ID="s54"/>
      <tc:sentence tokenIDs="w83b w83c w83d w83e w83f w840 w841 w842 w843 w844 w845 w846 w847 w848 w849 w84a w84b" ID="s55"/>
      <tc:sentence tokenIDs="w84c w84d w84e w84f w850 w851 w852 w853 w854" ID="s56"/>
      <tc:sentence tokenIDs="w855 w856 w857 w858 w859 w85a w85b w85c w85d w85e w85f w860 w861" ID="s57"/>
      <tc:sentence tokenIDs="w862 w863 w864 w865 w866 w867 w868 w869 w86a w86b w86c w86d w86e w86f" ID="s58"/>
      <tc:sentence tokenIDs="w870 w871 w872 w873 w874 w875 w876" ID="s59"/>
      <tc:sentence tokenIDs="w877 w878 w879 w87a w87b w87c w87d w87e w87f w880 w881 w882 w883" ID="s5a"/>
      <tc:sentence tokenIDs="w884 w885 w886 w887 w888 w889 w88a w88b w88c w88d w88e w88f w890 w891" ID="s5b"/>
      <tc:sentence tokenIDs="w892 w893 w894 w895 w896 w897 w898 w899 w89a w89b w89c" ID="s5c"/>
      <tc:sentence tokenIDs="w89d w89e w89f w8a0 w8a1 w8a2 w8a3 w8a4 w8a5 w8a6 w8a7 w8a8 w8a9 w8aa w8ab w8ac w8ad w8ae w8af w8b0 w8b1 w8b2 w8b3 w8b4 w8b5 w8b6 w8b7 w8b8" ID="s5d"/>
      <tc:sentence tokenIDs="w8b9 w8ba w8bb w8bc w8bd w8be w8bf" ID="s5e"/>
      <tc:sentence tokenIDs="w8c0 w8c1 w8c2 w8c3 w8c4 w8c5 w8c6" ID="s5f"/>
      <tc:sentence tokenIDs="w8c7 w8c8 w8c9" ID="s60"/>
      <tc:sentence tokenIDs="w8ca w8cb w8cc" ID="s61"/>
      <tc:sentence tokenIDs="w8cd w8ce w8cf w8d0 w8d1 w8d2 w8d3 w8d4 w8d5 w8d6 w8d7 w8d8 w8d9 w8da w8db w8dc w8dd w8de" ID="s62"/>
      <tc:sentence tokenIDs="w8df w8e0 w8e1 w8e2 w8e3" ID="s63"/>
      <tc:sentence tokenIDs="w8e4 w8e5 w8e6" ID="s64"/>
      <tc:sentence tokenIDs="w8e7 w8e8 w8e9 w8ea w8eb w8ec w8ed w8ee w8ef w8f0 w8f1 w8f2 w8f3 w8f4 w8f5 w8f6 w8f7 w8f8 w8f9 w8fa w8fb" ID="s65"/>
      <tc:sentence tokenIDs="w8fc w8fd w8fe w8ff w900 w901 w902 w903 w904 w905 w906 w907 w908 w909 w90a w90b w90c w90d w90e w90f w910 w911 w912 w913 w914 w915 w916 w917 w918 w919 w91a w91b w91c w91d" ID="s66"/>
      <tc:sentence tokenIDs="w91e w91f w920 w921 w922 w923 w924 w925 w926 w927 w928 w929 w92a w92b w92c w92d w92e w92f w930 w931 w932 w933 w934 w935 w936 w937 w938 w939 w93a w93b w93c w93d w93e w93f w940 w941 w942 w943 w944 w945 w946 w947 w948 w949 w94a w94b" ID="s67"/>
      <tc:sentence tokenIDs="w94c" ID="s68"/>
    </tc:sentences>
    <tc:namedEntities xmlns:tc="http://www.dspin.de/data/textcorpus" type="tuebadz8">
      <tc:entity class="PER" tokenIDs="w98"/>
      <tc:entity class="PER" tokenIDs="w171"/>
      <tc:entity class="OTH" tokenIDs="w19c w19d w19e"/>
      <tc:entity class="ORG" tokenIDs="w26c"/>
      <tc:entity class="PER" tokenIDs="w27f w280 w281"/>
      <tc:entity class="ORG" tokenIDs="w283"/>
      <tc:entity class="OTH" tokenIDs="w286 w287 w288 w289 w28a w28b w28c w28d w28e w28f w290 w291 w292 w293 w294 w295 w296 w297 w298 w299"/>
      <tc:entity class="OTH" tokenIDs="w29b w29c w29d"/>
      <tc:entity class="GPE" tokenIDs="w29f"/>
      <tc:entity class="ORG" tokenIDs="w3bc"/>
      <tc:entity class="ORG" tokenIDs="w6a3"/>
      <tc:entity class="ORG" tokenIDs="w6a5 w6a6 w6a7"/>
      <tc:entity class="PER" tokenIDs="w70e w70f"/>
      <tc:entity class="PER" tokenIDs="w712"/>
      <tc:entity class="OTH" tokenIDs="w855 w856 w857 w858 w859 w85a w85b w85c w85d w85e w85f w860"/>
      <tc:entity class="OTH" tokenIDs="w862 w863 w864 w865 w866 w867 w868 w869 w86a"/>
      <tc:entity class="OTH" tokenIDs="w892 w893 w894 w895 w896 w897 w898"/>
      <tc:entity class="OTH" tokenIDs="w8af w8b0 w8b1 w8b2 w8b3 w8b4"/>
      <tc:entity class="PER" tokenIDs="w8b9 w8ba w8bb"/>
      <tc:entity class="ORG" tokenIDs="w8c7 w8c8"/>
      <tc:entity class="GPE" tokenIDs="w8ca w8cb"/>
      <tc:entity class="PER" tokenIDs="w8d1"/>
      <tc:entity class="PER" tokenIDs="w8d3"/>
      <tc:entity class="PER" tokenIDs="w8d5 w8d6"/>
      <tc:entity class="PER" tokenIDs="w8d8"/>
      <tc:entity class="PER" tokenIDs="w8da"/>
      <tc:entity class="PER" tokenIDs="w8ec"/>
      <tc:entity class="PER" tokenIDs="w8ee w8ef"/>
      <tc:entity class="PER" tokenIDs="w8f1"/>
      <tc:entity class="PER" tokenIDs="w8f3"/>
      <tc:entity class="PER" tokenIDs="w8f5"/>
      <tc:entity class="PER" tokenIDs="w8f7"/>
      <tc:entity class="PER" tokenIDs="w911"/>
      <tc:entity class="PER" tokenIDs="w913"/>
      <tc:entity class="PER" tokenIDs="w915"/>
      <tc:entity class="PER" tokenIDs="w917"/>
      <tc:entity class="PER" tokenIDs="w919"/>
      <tc:entity class="OTH" tokenIDs="w927 w928 w929"/>
    </tc:namedEntities>
  </TextCorpus>
</D-Spin>